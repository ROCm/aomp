diff -Naur -x .git llvm-project.atd/clang/lib/CodeGen/CGBuiltin.cpp llvm-project/clang/lib/CodeGen/CGBuiltin.cpp
--- llvm-project.atd/clang/lib/CodeGen/CGBuiltin.cpp	2024-09-25 10:05:14.994414749 -0500
+++ llvm-project/clang/lib/CodeGen/CGBuiltin.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -5989,6 +5989,9 @@
            getTarget().getTriple().isSPIRV()) &&
           getLangOpts().HIP)
         return EmitAMDGPUDevicePrintfCallExpr(E);
+      else if (getLangOpts().OpenMP)
+        return EmitHostexecAllocAndExecFns(E, "hostexec_allocate",
+                                              "printf_execute");
     }
 
     break;
diff -Naur -x .git llvm-project.atd/clang/lib/CodeGen/CGExpr.cpp llvm-project/clang/lib/CodeGen/CGExpr.cpp
--- llvm-project.atd/clang/lib/CodeGen/CGExpr.cpp	2024-09-25 10:05:15.122417468 -0500
+++ llvm-project/clang/lib/CodeGen/CGExpr.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -6098,6 +6098,18 @@
       StaticOperator = true;
   }
 
+  // GPUs can execute hostexec variadic functions, printf, and fprintf on host.
+  if ((CGM.getTriple().isAMDGCN() || CGM.getTriple().isNVPTX()) &&
+      CGM.getLangOpts().OpenMP && FnType &&
+      dyn_cast<FunctionProtoType>(FnType) &&
+      dyn_cast<FunctionProtoType>(FnType)->isVariadic() &&
+      (std::find(std::begin(HostexecFns), std::end(HostexecFns),
+                 E->getDirectCallee()->getNameAsString()) !=
+       std::end(HostexecFns)))
+    return EmitHostexecAllocAndExecFns(
+        E, "hostexec_allocate",
+        E->getDirectCallee()->getNameAsString().append("_execute").c_str());
+
   auto Arguments = E->arguments();
   if (StaticOperator) {
     // If we're calling a static operator, we need to emit the object argument
diff -Naur -x .git llvm-project.atd/clang/lib/CodeGen/CGGPUBuiltin.cpp llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp
--- llvm-project.atd/clang/lib/CodeGen/CGGPUBuiltin.cpp	2024-09-25 10:05:15.242420016 -0500
+++ llvm-project/clang/lib/CodeGen/CGGPUBuiltin.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -191,3 +191,361 @@
   Builder.SetInsertPoint(IRB.GetInsertBlock(), IRB.GetInsertPoint());
   return RValue::get(Printf);
 }
+
+// EmitHostexecAllocAndExecFns:
+//
+// For printf in an OpenMP Target region on amdgcn and for variable argument
+// functions that have a supporting host service function struct
+// is created to represent the vargs for each call site.
+// The struct contains the length, number of args, an array of 4-byte keys
+// that represent the type of of each arg, an array of aligned "data" values
+// for each arg, and finally the runtime string values. If an arg is a string
+// the data value is the runtime length of the string.  Each 4-byte key
+// contains the llvm type ID and the number of bits for the type.
+// encoded by the macro PACK_TY_BITLEN(x,y) ((uint32_t)x << 16) | ((uint32_t)y)
+// The llvm type ID of a string is pointer. To distinguish string pointers
+// from non-string pointers, the number of bitlen is set to 1.
+//
+// For example, here is a 4 arg printf function
+//
+// printf("format string %d %s %f \n", (int) 1, "string2", (double) 1.234);
+//
+// is represented by a struct with these 13 elements.
+//
+//  {81, 4, 983041, 720928, 983041, 196672, 25, int 1, 7, 0, double 1.234,
+//     "format string %d %s %ld\n", "string2" }
+//
+// 81 is the total length of the buffer that must be allocated.
+// 4 is the number of arguments.
+// The next 4 key values represent the data types of the 4 args.
+// The format string length is 25.
+// The integer field is next.
+// The string argument "string2" has length 7
+// The 4-byte dummy arg 0 is inserted so the next double arg is aligned.
+// The string arguments follows the header, keys, and data args.
+//
+// Before the struct is written, a hostexec alloc call is emitted to allocate
+// memory for the transfer. Then the struct is emitted.  Then a call
+// to the execute the GPU stub function that initiates the service
+// on the host.  The host runtime passes the buffer to the service routine
+// for processing.
+
+// These static helper functions support EmitHostexecAllocAndExecFns.
+
+// For strings that vary in length at runtime this strlen_max
+// will stop at a provided maximum.
+static llvm::Function *GetOmpStrlenDeclaration(CodeGenModule &CGM) {
+  auto &M = CGM.getModule();
+  // Args are pointer to char and maxstringlen
+  llvm::Type *ArgTypes[] = {CGM.Int8PtrTy, CGM.Int32Ty};
+  llvm::FunctionType *OmpStrlenFTy =
+      llvm::FunctionType::get(CGM.Int32Ty, ArgTypes, false);
+  if (auto *F = M.getFunction("__strlen_max")) {
+    assert(F->getFunctionType() == OmpStrlenFTy);
+    return F;
+  }
+  llvm::Function *FN = llvm::Function::Create(
+      OmpStrlenFTy, llvm::GlobalVariable::ExternalLinkage, "__strlen_max", &M);
+  return FN;
+}
+
+// Deterimines if an expression is a string with variable lenth
+static bool isVarString(const clang::Expr *argX, const clang::Type *argXTy,
+                        const llvm::Value *Arg) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType() && !argX->isLValue())
+    return true;
+  // Ensure the VarDecl has an inititalizer
+  if (const auto *DRE = dyn_cast<DeclRefExpr>(argX))
+    if (const auto *VD = dyn_cast<VarDecl>(DRE->getDecl()))
+      if (!VD->getInit() ||
+          !llvm::isa<StringLiteral>(VD->getInit()->IgnoreImplicit()))
+        return true;
+  return false;
+}
+
+// Deterimines if an argument is a string
+static bool isString(const clang::Type *argXTy) {
+  if ((argXTy->isPointerType() || argXTy->isConstantArrayType()) &&
+      argXTy->getPointeeOrArrayElementType()->isCharType())
+    return true;
+  else
+    return false;
+}
+
+// Gets a string literal to write into the transfer buffer
+static const StringLiteral *getSL(const clang::Expr *argX,
+                                  const clang::Type *argXTy) {
+  // String in argX has known constant length
+  if (!argXTy->isConstantArrayType()) {
+    // Allow constant string to be a declared variable,
+    // But it must be constant and initialized.
+    const DeclRefExpr *DRE = cast<DeclRefExpr>(argX);
+    const VarDecl *VarD = cast<VarDecl>(DRE->getDecl());
+    argX = VarD->getInit()->IgnoreImplicit();
+  }
+  const StringLiteral *SL = cast<StringLiteral>(argX);
+  return SL;
+}
+
+// Returns a function pointer to the memory allocation routine
+static llvm::Function *GetVargsFnAllocDeclaration(CodeGenModule &CGM,
+                                                  const char *GPUAllocateName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VargsFnAllocFuncType = llvm::FunctionType::get(
+      llvm::PointerType::getUnqual(CGM.Int8Ty), ArgTypes, false);
+
+  if (!(FN = M.getFunction(GPUAllocateName)))
+    FN = llvm::Function::Create(VargsFnAllocFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUAllocateName, &M);
+  assert(FN->getFunctionType() == VargsFnAllocFuncType);
+  return FN;
+}
+
+// Returns a function pointer to the GPU stub function
+static llvm::Function *
+hostexecVargsReturnsFnDeclaration(CodeGenModule &CGM, QualType Ty,
+                                  const char *GPUStubFunctionName) {
+  auto &M = CGM.getModule();
+  llvm::Type *ArgTypes[] = {llvm::PointerType::getUnqual(CGM.Int8Ty),
+                            CGM.Int32Ty};
+  llvm::Function *FN;
+  llvm::FunctionType *VarfnFuncType =
+      llvm::FunctionType::get(CGM.getTypes().ConvertType(Ty), ArgTypes, false);
+  if (!(FN = M.getFunction(GPUStubFunctionName)))
+    FN = llvm::Function::Create(VarfnFuncType,
+                                llvm::GlobalVariable::ExternalLinkage,
+                                GPUStubFunctionName, &M);
+  assert(FN->getFunctionType() == VarfnFuncType);
+  return FN;
+}
+
+// The macro to pack the llvm type ID and numbits into 4-byte key
+#define PACK_TY_BITLEN(x, y) ((uint32_t)x << 16) | ((uint32_t)y)
+
+// Emit the code to support a host vargs function such as printf.
+RValue
+CodeGenFunction::EmitHostexecAllocAndExecFns(const CallExpr *E,
+                                             const char *GPUAllocateName,
+                                             const char *GPUStubFunctionName) {
+  assert(getTarget().getTriple().isAMDGCN() ||
+         getTarget().getTriple().isNVPTX());
+  // assert(E->getBuiltinCallee() == Builtin::BIprintf);
+  assert(E->getNumArgs() >= 1); // hostexec always has at least one arg.
+
+  const llvm::DataLayout &DL = CGM.getDataLayout();
+
+  CallArgList Args;
+  EmitCallArgs(Args,
+               E->getDirectCallee()->getType()->getAs<FunctionProtoType>(),
+               E->arguments(), E->getDirectCallee(),
+               /* ParamsToSkip = */ 0);
+
+  // We don't know how to emit non-scalar varargs.
+  if (std::any_of(Args.begin() + 1, Args.end(), [&](const CallArg &A) {
+        return !A.getRValue(*this).isScalar();
+      })) {
+    CGM.ErrorUnsupported(E, "non-scalar arg in GPU vargs function");
+    return RValue::get(llvm::ConstantInt::get(IntTy, 0));
+  }
+
+  unsigned NumArgs = (unsigned)Args.size();
+  llvm::SmallVector<llvm::Type *, 32> ArgTypes;
+  llvm::SmallVector<llvm::Value *, 32> VarStrLengths;
+  llvm::Value *TotalVarStrsLength = llvm::ConstantInt::get(Int32Ty, 0);
+  bool hasVarStrings = false;
+  ArgTypes.push_back(Int32Ty); // First field in struct will be total DataLen
+  ArgTypes.push_back(Int32Ty); // 2nd field in struct will be num args
+  // An array of 4-byte keys that describe the arg type
+  for (unsigned I = 0; I < NumArgs; ++I)
+    ArgTypes.push_back(Int32Ty);
+
+  // Track the size of the numeric data length and string length
+  unsigned DataLen_CT =
+      (unsigned)(DL.getTypeAllocSize(Int32Ty)) * (NumArgs + 2);
+  unsigned AllStringsLen_CT = 0;
+
+  // ---  1st Pass over Args to create ArgTypes and count size ---
+
+  size_t structOffset = 4 * (NumArgs + 2);
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg = Args[I].getRValue(*this).getScalarVal();
+    llvm::Type *ArgType = Arg->getType();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        hasVarStrings = true;
+        if (auto *PtrTy = dyn_cast<llvm::PointerType>(ArgType))
+          if (PtrTy->getPointerAddressSpace()) {
+            Arg = Builder.CreateAddrSpaceCast(Arg, CGM.Int8PtrTy);
+            ArgType = Arg->getType();
+          }
+        llvm::Value *VarStrLen =
+            Builder.CreateCall(GetOmpStrlenDeclaration(CGM),
+                               {Arg, llvm::ConstantInt::get(Int32Ty, 1024)});
+        VarStrLengths.push_back(VarStrLen);
+        TotalVarStrsLength = Builder.CreateAdd(TotalVarStrsLength, VarStrLen,
+                                               "sum_of_var_strings_length");
+        ArgType = Int32Ty;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        AllStringsLen_CT += ((int)ArgString.size() + 1);
+        // change ArgType from char ptr to int to contain string length
+        ArgType = Int32Ty;
+      }
+    } // end of processing string argument
+    // if ArgTypeSize is >4 bytes we need to insert dummy align
+    // values in the struct so all stores can be aligned .
+    // These dummy fields must be inserted before the arg.
+    //
+    // In the pass below where the stores are generated careful
+    // tracking of the index into the struct is necessary.
+    size_t needsPadding = (structOffset % (size_t)DL.getTypeAllocSize(ArgType));
+    if (needsPadding) {
+      DataLen_CT += (unsigned)needsPadding;
+      structOffset += needsPadding;
+      ArgTypes.push_back(Int32Ty); // should assert that needsPadding == 4 here
+    }
+
+    ArgTypes.push_back(ArgType);
+    DataLen_CT += ((int)DL.getTypeAllocSize(ArgType));
+    structOffset += (size_t)DL.getTypeAllocSize(ArgType);
+  }
+
+  // ---  Generate call to printf_alloc to get pointer to data structure  ---
+  if (hasVarStrings)
+    TotalVarStrsLength = Builder.CreateAdd(
+        TotalVarStrsLength,
+        llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT),
+        "total_buffer_size");
+  llvm::Value *BufferLen =
+      hasVarStrings
+          ? TotalVarStrsLength
+          : llvm::ConstantInt::get(Int32Ty, AllStringsLen_CT + DataLen_CT);
+
+  llvm::Value *DataStructPtr = Builder.CreateCall(
+      GetVargsFnAllocDeclaration(CGM, GPUAllocateName), {BufferLen});
+
+  // cast the generic return pointer to be a struct in device global memory
+  llvm::StructType *DataStructTy =
+      llvm::StructType::create(ArgTypes, "varfn_args_store");
+  unsigned AS = getContext().getTargetAddressSpace(LangAS::cuda_device);
+  llvm::Value *BufferPtr = Builder.CreatePointerCast(
+      DataStructPtr, llvm::PointerType::get(DataStructTy, AS),
+      "varfn_args_store_casted");
+
+  // ---  Header of struct contains length and NumArgs ---
+  llvm::Value *DataLenField = llvm::ConstantInt::get(Int32Ty, DataLen_CT);
+  llvm::Value *P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 0);
+  Builder.CreateAlignedStore(
+      DataLenField, P, DL.getPrefTypeAlign(DataLenField->getType()));
+  llvm::Value *NumArgsField = llvm::ConstantInt::get(Int32Ty, NumArgs);
+  P = Builder.CreateStructGEP(DataStructTy, BufferPtr, 1);
+  Builder.CreateAlignedStore(
+      NumArgsField, P, DL.getPrefTypeAlign(NumArgsField->getType()));
+
+  // ---  2nd Pass: create array of 4-byte keys to describe each arg
+
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Type *ty = Args[I].getRValue(*this).getScalarVal()->getType();
+    llvm::Type::TypeID argtypeid =
+        Args[I].getRValue(*this).getScalarVal()->getType()->getTypeID();
+
+    // Get type size in bits. Usually 64 or 32.
+    uint32_t numbits = 0;
+    if (isString(E->getArg(I)->IgnoreParenCasts()->getType().getTypePtr()))
+      // The llvm typeID for string is pointer.  Since pointer numbits is 0,
+      // we set numbits to 1 to distinguish pointer type ID as string pointer.
+      numbits = 1;
+    else
+      numbits = ty->getScalarSizeInBits();
+    // Create a key that combines llvm typeID and size
+    llvm::Value *Key =
+        llvm::ConstantInt::get(Int32Ty, PACK_TY_BITLEN(argtypeid, numbits));
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, I + 2);
+    Builder.CreateAlignedStore(Key, P, DL.getPrefTypeAlign(Key->getType()));
+  }
+
+  // ---  3rd Pass: Store thread-specfic data values for each arg ---
+
+  unsigned varstring_index = 0;
+  unsigned structIndex = 2 + NumArgs;
+  structOffset = 4 * structIndex;
+  for (unsigned I = 0; I < NumArgs; I++) {
+    llvm::Value *Arg;
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        Arg = VarStrLengths[varstring_index];
+        varstring_index++;
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        // Change Arg from a char pointer to the integer string length
+        Arg = llvm::ConstantInt::get(Int32Ty, ArgStrLen);
+      }
+    } else {
+      Arg = Args[I].getKnownRValue().getScalarVal();
+    }
+    size_t structElementSize = (size_t)DL.getTypeAllocSize(Arg->getType());
+    size_t needsPadding = (structOffset % structElementSize);
+    if (needsPadding) {
+      // Skip over dummy fields in struct to align
+      structOffset += needsPadding; // should assert needsPadding == 4
+      structIndex++;
+    }
+    P = Builder.CreateStructGEP(DataStructTy, BufferPtr, structIndex);
+    Builder.CreateAlignedStore(Arg, P, DL.getPrefTypeAlign(Arg->getType()));
+    structOffset += structElementSize;
+    structIndex++;
+  }
+
+  // ---  4th Pass: memcpy all strings after the data values ---
+
+  // bitcast the struct in device global memory as a char buffer
+  Address BufferPtrByteAddr = Address(
+      Builder.CreatePointerCast(BufferPtr, llvm::PointerType::get(Int8Ty, AS)),
+      Int8Ty, CharUnits::fromQuantity(1));
+  // BufferPtrByteAddr is a pointer to where we want to write the next string
+  BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+      BufferPtrByteAddr, CharUnits::fromQuantity(DataLen_CT));
+  varstring_index = 0;
+  for (unsigned I = 0; I < NumArgs; ++I) {
+    llvm::Value *Arg = Args[I].getKnownRValue().getScalarVal();
+    const Expr *argX = E->getArg(I)->IgnoreParenCasts();
+    auto *argXTy = argX->getType().getTypePtr();
+    if (isString(argXTy)) {
+      if (isVarString(argX, argXTy, Arg)) {
+        llvm::Value *varStrLength = VarStrLengths[varstring_index];
+        varstring_index++;
+        Address SrcAddr = Address(Arg, Int8Ty, CharUnits::fromQuantity(1));
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, varStrLength);
+        // update BufferPtrByteAddr for next string memcpy
+        llvm::Value *PtrAsInt = BufferPtrByteAddr.emitRawPointer(*this);
+        BufferPtrByteAddr = Address(
+            Builder.CreateGEP(Int8Ty,
+		    PtrAsInt, ArrayRef<llvm::Value*>(varStrLength)),
+            Int8Ty, CharUnits::fromQuantity(1));
+      } else {
+        const StringLiteral *SL = getSL(argX, argXTy);
+        StringRef ArgString = SL->getString();
+        int ArgStrLen = (int)ArgString.size() + 1;
+        Address SrcAddr = CGM.GetAddrOfConstantStringFromLiteral(SL);
+        Builder.CreateMemCpy(BufferPtrByteAddr, SrcAddr, ArgStrLen);
+        // update BufferPtrByteAddr for next memcpy
+        BufferPtrByteAddr = Builder.CreateConstInBoundsByteGEP(
+            BufferPtrByteAddr, CharUnits::fromQuantity(ArgStrLen));
+      }
+    }
+  }
+  return RValue::get(Builder.CreateCall(
+      hostexecVargsReturnsFnDeclaration(CGM, E->getType(), GPUStubFunctionName),
+      {DataStructPtr, BufferLen}));
+}
diff -Naur -x .git llvm-project.atd/clang/lib/CodeGen/CodeGenFunction.h llvm-project/clang/lib/CodeGen/CodeGenFunction.h
--- llvm-project.atd/clang/lib/CodeGen/CodeGenFunction.h	2024-09-25 10:05:15.358422479 -0500
+++ llvm-project/clang/lib/CodeGen/CodeGenFunction.h	2024-09-25 08:28:31.616214109 -0500
@@ -4548,6 +4548,13 @@
 
   RValue EmitNVPTXDevicePrintfCallExpr(const CallExpr *E);
   RValue EmitAMDGPUDevicePrintfCallExpr(const CallExpr *E);
+  std::vector<std::string> HostexecFns{
+      "printf",          "fprintf",        "hostexec",      "hostexec_uint",
+      "hostexec_uint64", "hostexec_int",   "hostexec_long", "hostexec_float",
+      "hostexec_double", "hostexec_fortrt"};
+  RValue EmitHostexecAllocAndExecFns(const CallExpr *E,
+                                     const char *allocate_name,
+                                     const char *execute_name);
 
   RValue EmitBuiltinExpr(const GlobalDecl GD, unsigned BuiltinID,
                          const CallExpr *E, ReturnValueSlot ReturnValue);
diff -Naur -x .git llvm-project.atd/offload/DeviceRTL/CMakeLists.txt llvm-project/offload/DeviceRTL/CMakeLists.txt
--- llvm-project.atd/offload/DeviceRTL/CMakeLists.txt	2024-09-25 10:05:15.478425028 -0500
+++ llvm-project/offload/DeviceRTL/CMakeLists.txt	2024-09-25 08:28:31.616214109 -0500
@@ -83,6 +83,7 @@
   ${include_directory}/DeviceTypes.h
   ${include_directory}/DeviceUtils.h
   ${include_directory}/Workshare.h
+  ${include_directory}/Hostexec.h
 )
 
 set(src_files
@@ -101,6 +102,7 @@
   ${source_directory}/Tasking.cpp
   ${source_directory}/DeviceUtils.cpp
   ${source_directory}/Workshare.cpp
+  ${source_directory}/Hostexec.cpp
 )
 
 # We disable the slp vectorizer during the runtime optimization to avoid
@@ -140,6 +142,29 @@
   set(target_bc_flags ${ARGN})
 
   set(bc_files "")
+  set(ocl_atomics_cl_filename ${CMAKE_CURRENT_SOURCE_DIR}/src/oclAtomics.cl)
+  set(ocl_atomics_bc_filename ${CMAKE_CURRENT_BINARY_DIR}/ocl_atomics_${target_name}_${target_cpu}.bc)
+  if (${target_name} STREQUAL "amdgpu")
+    set(depfile "${ocl_atomics_bc_filename}.d")
+    set(opencl_cmd ${CLANG_TOOL}
+       -fvisibility=default
+       -c -emit-llvm -nogpulib -nogpuinc
+       -DCL_VERSION_2_0=200 -D__OPENCL_C_VERSION__=200
+       -Dcl_khr_fp64 -Dcl_khr_fp16
+       -Dcl_khr_subgroups -Dcl_khr_int64_base_atomics -Dcl_khr_int64_extended_atomics
+       -x cl -Xclang -cl-std=CL2.0 -Xclang -finclude-default-header
+       -Xclang -mcode-object-version=none
+       -MD -MF ${depfile}
+       -target amdgcn-amd-amdhsa )
+    add_custom_command(OUTPUT ${ocl_atomics_bc_filename}
+      COMMAND ${opencl_cmd} ${ocl_atomics_cl_filename} -o ${ocl_atomics_bc_filename}
+      DEPENDS ${ocl_atomics_cl_filename}
+      DEPFILE ${depfile}
+      COMMENT "CL Building ${ocl_atomics_bc_filename}"
+      VERBATIM)
+    list(APPEND bc_files ${ocl_atomics_bc_filename})
+  endif()
+
   foreach(src ${src_files})
     get_filename_component(infile ${src} ABSOLUTE)
     get_filename_component(outfile ${src} NAME)
diff -Naur -x .git llvm-project.atd/offload/DeviceRTL/include/Hostexec.h llvm-project/offload/DeviceRTL/include/Hostexec.h
--- llvm-project.atd/offload/DeviceRTL/include/Hostexec.h	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/DeviceRTL/include/Hostexec.h	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,80 @@
+#ifndef __HOSTEXEC_DEV_H__
+#define __HOSTEXEC_DEV_H__
+
+// This device runtime utility function is needed for variable length strings.
+// EXTERN uint32_t __strlen_max(char *instr, uint32_t maxstrlen);
+
+// The host runtime for host services is linked statically into the
+// libomptarget plugin i.e.  libomptarget.rtl.amdgcn or libomptarget.rtl.cuda
+// Typically these are part of the compiler installation so VRM checking
+// would not be necessary. However, compiled applications that dynamically
+// link to libomptarget and the plugin may might get an old runtime.
+// so VRM checking is
+//
+// Please update at least the patch level when adding a new service id (sid)
+// below. This ensures that applications that use a new device stub do not
+// try to use backlevel host runtimes that do not have a valid VRM.
+enum hostexec_sid {
+  HOSTEXEC_SID_UNUSED,
+  HOSTEXEC_SID_TERMINATE,
+  HOSTEXEC_SID_DEVICE_MALLOC, // Device global memory
+  HOSTEXEC_SID_HOST_MALLOC,   // shared or managed memory
+  HOSTEXEC_SID_FREE,
+  HOSTEXEC_SID_PRINTF,
+  HOSTEXEC_SID_FPRINTF,
+  HOSTEXEC_SID_FTNASSIGN,
+  HOSTEXEC_SID_SANITIZER,
+  HOSTEXEC_SID_UINT,
+  HOSTEXEC_SID_UINT64,
+  HOSTEXEC_SID_DOUBLE,
+  HOSTEXEC_SID_INT,
+  HOSTEXEC_SID_LONG,
+  HOSTEXEC_SID_FLOAT,
+  HOSTEXEC_SID_VOID,
+  HOSTEXEC_SID_FORTRT,
+};
+
+typedef enum fortran_device_rt_idx {
+  _FortranAio_INVALID,
+  _FortranAioBeginExternalListOutput_idx,
+  _FortranAioOutputAscii_idx,
+  _FortranAioOutputInteger32_idx,
+  _FortranAioEndIoStatement_idx,
+  _FortranAioOutputInteger8_idx,
+  _FortranAioOutputInteger16_idx,
+  _FortranAioOutputInteger64_idx,
+  _FortranAioOutputReal32_idx,
+  _FortranAioOutputReal64_idx,
+  _FortranAioOutputComplex32_idx,
+  _FortranAioOutputComplex64_idx,
+  _FortranAioOutputLogical_idx,
+  _FortranAAbort_idx,
+  _FortranAStopStatementText_idx,
+} fortran_device_rt_idx;
+
+#if 0
+typedef void hostexec_t(void *, ...);
+typedef uint32_t hostexec_uint_t(void *, ...);
+typedef uint64_t hostexec_uint64_t(void *, ...);
+typedef double hostexec_double_t(void *, ...);
+typedef float hostexec_float_t(void *, ...);
+typedef int hostexec_int_t(void *, ...);
+typedef long hostexec_long_t(void *, ...);
+#endif
+
+#if defined(__NVPTX__) || defined(__AMDGCN__)
+
+// Device interfaces for user-callable hostexec functions
+extern "C" void hostexec(void *fnptr, ...);
+extern "C" uint32_t hostexec_uint(void *fnptr, ...);
+extern "C" uint64_t hostexec_uint64(void *fnptr, ...);
+extern "C" uint64_t hostexec_fortrt(void *fnptr, ...);
+extern "C" double hostexec_double(void *fnptr, ...);
+extern "C" float hostexec_float(void *fnptr, ...);
+extern "C" int hostexec_int(void *fnptr, ...);
+extern "C" long hostexec_long(void *fnptr, ...);
+
+#endif
+
+
+#endif // __HOSTEXEC_DEV_H__ 
diff -Naur -x .git llvm-project.atd/offload/DeviceRTL/src/exports llvm-project/offload/DeviceRTL/src/exports
--- llvm-project.atd/offload/DeviceRTL/src/exports	2024-09-25 10:05:15.594427491 -0500
+++ llvm-project/offload/DeviceRTL/src/exports	2024-09-25 08:28:31.616214109 -0500
@@ -14,5 +14,8 @@
 malloc
 free
 memcmp
-printf
+printf_execute
+fprintf_execute
+_Fortran*
+hostexec_*
 __assert_fail
diff -Naur -x .git llvm-project.atd/offload/DeviceRTL/src/Hostexec.cpp llvm-project/offload/DeviceRTL/src/Hostexec.cpp
--- llvm-project.atd/offload/DeviceRTL/src/Hostexec.cpp	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/DeviceRTL/src/Hostexec.cpp	2024-09-25 09:56:37.787596236 -0500
@@ -0,0 +1,769 @@
+//===----- Hostexec.cpp -  OpenMP workshare implementation ------ C++ -*-===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the implementation of hostexec_invoke.  Hostexec_invoke
+// is called by device stubs to synchronously execute a host service request.
+// THis file also contains all usages of hostexec_invoke.  
+//
+//===----------------------------------------------------------------------===//
+
+#include "Synchronization.h"
+#include "DeviceTypes.h"
+#include "Hostexec.h"
+
+/** \brief 
+ *  hostexec_invoke is a wave-wide operation, where the service_id
+ *  must be uniform, but the parameters are different for each
+ *  workitem. Parameters from all active lanes are written into
+ *  the host packet (buffer). hostexec_invoke blocks until the host
+ *  processes the request, and returns the response it receives.
+ *
+ *  service_thread_buf is a global constant symbol that contains the
+ *  pointer to the buffer used by the service thread. This is written
+ *  by nextgen plugin method writeGlobalToDevice only when the device
+ *  image requires host services.
+ *  This is the alternative to using reserved IMPLICIT kern arg hostcall
+ *  because nvptx arch does not have implicit kern args.
+ */
+
+using namespace ompx;
+
+#define GLOB_ATTR __attribute__((address_space(1)))
+#define __static_inl static __attribute__((flatten, always_inline))
+#define __inl __attribute__((flatten, always_inline))
+
+// headers for amdgcn opencl (ocl) atomics
+typedef uint64_t hsa_signal_value_t;
+typedef uint64_t hsa_signal_t;
+// typedef struct {
+//  uint64_t handle;
+// } signal_t;
+// mem order codes: A=acquire, X=relaxed, R=release
+typedef enum __ockl_memory_order_e {
+  __ockl_memory_order_relaxed = __ATOMIC_RELAXED,
+  __ockl_memory_order_acquire = __ATOMIC_ACQUIRE,
+  __ockl_memory_order_release = __ATOMIC_RELEASE,
+  __ockl_memory_order_acq_rel = __ATOMIC_ACQ_REL,
+  __ockl_memory_order_seq_cst = __ATOMIC_SEQ_CST,
+} __ockl_memory_order;
+extern "C" __inl uint64_t oclAtomic64Load_A(GLOB_ATTR uint64_t *Address);
+extern "C" __inl uint64_t oclAtomic64Load_X(GLOB_ATTR uint64_t *Address);
+extern "C" __inl uint32_t oclAtomic32Load_A(GLOB_ATTR const uint32_t *Address);
+extern "C" __inl uint32_t oclAtomic64CAS_AX(GLOB_ATTR uint64_t *Address,
+                                            uint64_t *e_Val, uint64_t new_ptr);
+extern "C" __inl uint32_t oclAtomic64CAS_RX(GLOB_ATTR uint64_t *Address,
+                                            uint64_t *e_Val, uint64_t new_ptr);
+extern "C" void ocl_hsa_signal_add(hsa_signal_t signal,
+                             hsa_signal_value_t value, __ockl_memory_order mo);
+
+// headers for cuda nvptx atomics
+extern "C" __attribute__((nothrow)) unsigned long long
+__ullAtomicAdd_system(unsigned long long *address, unsigned long long val);
+extern "C" __attribute__((nothrow)) unsigned __uAtomicAdd(unsigned *address,
+                                                          unsigned val);
+extern "C" __attribute__((nothrow)) unsigned long long
+__ullAtomicCAS_system(unsigned long long int *address,
+                      unsigned long long int compare,
+                      unsigned long long int val);
+// headers for builtins
+int __builtin_popcountl(unsigned long);
+int __builtin_popcount(unsigned);
+int __builtin_amdgcn_readfirstlane(int);
+unsigned long __builtin_amdgcn_read_exec();
+unsigned int __builtin_amdgcn_mbcnt_hi(unsigned int, unsigned int);
+unsigned int __builtin_amdgcn_mbcnt_lo(unsigned int, unsigned int);
+int __nvvm_read_ptx_sreg_tid_x();
+
+#pragma omp begin declare target device_type(nohost)
+
+typedef enum { STATUS_SUCCESS, STATUS_BUSY } status_t;
+
+typedef enum {
+  CONTROL_OFFSET_READY_FLAG = 0,
+  CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+typedef enum {
+  CONTROL_WIDTH_READY_FLAG = 1,
+  CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+typedef uint64_t LaneMask_t;
+
+typedef struct {
+  uint64_t next;
+  LaneMask_t activemask;
+  uint32_t service;
+  uint32_t control;
+} header_t;
+
+typedef struct {
+  // 64 slots of 8 uint64_ts each (4KB/payload)
+  uint64_t slots[64][8];
+} payload_t;
+
+
+typedef struct {
+  GLOB_ATTR header_t *headers;
+  GLOB_ATTR payload_t *payloads;
+  uint64_t doorbell;
+  uint64_t free_stack;
+  uint64_t ready_stack;
+  uint32_t index_size;
+  uint32_t device_id;
+} buffer_t;
+
+namespace impl {
+
+// These functions have arch-specific variants
+__inl void deviceSleepHostWait();
+//  These still dont have nvptx variants
+__inl void send_signal(uint64_t signal);
+__inl uint32_t first_lane_id(uint32_t val);
+__inl uint32_t lane_id();
+__inl uint64_t get_mask();
+// mem order codes: A=acquire, X=relaxed, R=release
+__inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address);
+__inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address);
+__inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address);
+__inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                          uint64_t new_ptr);
+__inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                          uint64_t new_ptr);
+__inl void write_needs_host_services_symbol();
+
+#pragma omp begin declare variant match(device = {arch(amdgcn)})
+
+__static_inl void write_needs_host_services_symbol() {
+  // The global variable "__needs_host_services" is used to detect that
+  // host services are required. If hostexec_invoke is not called, the symbol
+  // will not be present and the runtime can avoid allocating and initialising
+  // service_thread_buf.
+  __asm__(".type __needs_host_services,@object\n\t"
+          ".global __needs_host_services\n\t"
+          ".comm __needs_host_services,4" ::
+              :);
+  
+}
+
+__static_inl uint32_t lane_id() {
+  return __builtin_amdgcn_mbcnt_hi(~0u, __builtin_amdgcn_mbcnt_lo(~0u, 0u));
+};
+
+__static_inl uint32_t first_lane_id(uint32_t me) {
+  return __builtin_amdgcn_readfirstlane(me);
+}
+__static_inl uint64_t get_mask() { return __builtin_amdgcn_read_exec(); }
+__static_inl void send_signal(uint64_t signal) {
+  ocl_hsa_signal_add((hsa_signal_t) signal, 1, __ockl_memory_order_release);
+//  ompx::atomic::add(&(signal.handle), 1, ompx::atomic::release);
+}
+__static_inl void deviceSleepHostWait() { __builtin_amdgcn_s_sleep(1); }
+__static_inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_A(Address);
+  // return (uint64_t) ompx::atomic::load((uint64_t*) Address, ompx::atomic::aquire); 
+}
+__static_inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  return oclAtomic64Load_X(Address);
+  //return (uint64_t) ompx::atomic::load((uint64_t*) Address, ompx::atomic::relaxed);
+}
+__static_inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return oclAtomic32Load_A(Address);
+  //return (uint32_t) ompx::atomic::load((uint32_t*) Address, ompx::atomic::aquire);
+}
+__static_inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                 uint64_t new_ptr) {
+  return (bool)oclAtomic64CAS_AX(Address, e_Val, new_ptr);
+  // return (bool) ompx::atomic::cas((uint64_t*) Address, *e_Val, new_ptr,
+// 			 ompx::atomic::aquire, ompx::atomic::relaxed);
+}
+__static_inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                 uint64_t new_ptr) {
+  return (bool)oclAtomic64CAS_RX(Address, e_Val, new_ptr);
+  //return (bool) ompx::atomic::cas((uint64_t*) Address, *e_Val, new_ptr,
+//			 ompx::atomic::release, ompx::atomic::relaxed);
+}
+
+#pragma omp end declare variant
+
+#pragma omp begin declare variant match(                                       \
+        device = {arch(nvptx, nvptx64)},                                       \
+            implementation = {extension(match_any)})
+
+__static_inl void write_needs_host_services_symbol() {
+  // The global variable "__needs_host_services" is used to detect that
+  // host services are required. If hostexec_invoke is not called, the symbol
+  // will not be present and the runtime can avoid allocating and initialising
+  // service_thread_buf.
+  __asm__(".global .align 4 .u32 __needs_host_services = 1;");
+}
+__static_inl inline void send_signal(hsa_signal_t signal) {
+  __ullAtomicAdd_system((unsigned long long *)&signal, 1);
+}
+
+__static_inl void deviceSleepHostWait() {
+  int32_t start = __nvvm_read_ptx_sreg_clock();
+  for (;;) {
+    if ((__nvvm_read_ptx_sreg_clock() - start) >= 1000)
+      break;
+  }
+}
+
+__static_inl uint64_t get_mask() {
+  unsigned int Mask;
+  asm("activemask.b32 %0;" : "=r"(Mask));
+  uint64_t mask64 = (uint64_t)Mask;
+  return mask64;
+}
+
+//  FIXME: nvptx needs to use lane_id somehow here
+__static_inl uint32_t first_lane_id(unsigned int lane_id) {
+  unsigned int mask = (unsigned int)get_mask();
+  if (mask == 0)
+    return 0;
+  unsigned int pos = 0;
+  unsigned int m = 1;
+  while (!(mask & m)) {
+    m = m << 1;
+    pos++;
+  }
+  return pos;
+};
+
+__static_inl uint32_t lane_id() {
+  return (uint32_t)(__nvvm_read_ptx_sreg_tid_x() & 31);
+};
+
+__static_inl uint64_t atomic64Load_A(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result =
+      __ullAtomicAdd_system((unsigned long long *)Address, 0);
+  return (uint64_t)result;
+}
+__static_inl uint64_t atomic64Load_X(GLOB_ATTR uint64_t *Address) {
+  unsigned long long result =
+      __ullAtomicAdd_system((unsigned long long *)Address, 0);
+  return (uint64_t)result;
+}
+__static_inl uint32_t atomic32Load_A(GLOB_ATTR const uint32_t *Address) {
+  return __uAtomicAdd((uint32_t *)Address, 0);
+}
+__static_inl bool atomic64CAS_AX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                 uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system(
+      (unsigned long long *)Address, (unsigned long long)*e_Val,
+      (unsigned long long)new_ptr);
+  return (bool)result;
+}
+__static_inl bool atomic64CAS_RX(GLOB_ATTR uint64_t *Address, uint64_t *e_Val,
+                                 uint64_t new_ptr) {
+  unsigned long long result = __ullAtomicCAS_system(
+      (unsigned long long *)Address, (unsigned long long)*e_Val,
+      (unsigned long long)new_ptr);
+  return (bool)result;
+}
+
+#pragma omp end declare variant
+
+} // end namespace impl
+
+__static_inl uint64_t get_ptr_index(uint64_t ptr, uint32_t index_size) {
+  return ptr & (((uint64_t)1 << index_size) - 1);
+}
+
+__static_inl GLOB_ATTR header_t *get_header(GLOB_ATTR buffer_t *buffer,
+                                            uint64_t ptr) {
+  return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+
+__static_inl GLOB_ATTR payload_t *get_payload(GLOB_ATTR buffer_t *buffer,
+                                              uint64_t ptr) {
+  return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+// get_control_field only used by get_ready_flag
+__static_inl uint32_t get_control_field(uint32_t control, uint32_t offset,
+                                        uint32_t width) {
+  return (control >> offset) & ((1 << width) - 1);
+}
+
+// get_ready_flag only called by lead lane of get_return_value
+//                on atomically loaded control field of packet header
+__static_inl uint32_t get_ready_flag(uint32_t control) {
+  return get_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG);
+}
+
+// set_control_field only used by set_ready_flag
+__static_inl uint32_t set_control_field(uint32_t control, uint32_t offset,
+                                        uint32_t width, uint32_t value) {
+  uint32_t mask = ~(((1 << width) - 1) << offset);
+  return (control & mask) | (value << offset);
+}
+
+__static_inl uint32_t set_ready_flag(uint32_t control) {
+  return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG, 1);
+}
+
+__static_inl uint64_t pop(GLOB_ATTR uint64_t *top, GLOB_ATTR buffer_t *buffer) {
+  uint64_t F = impl::atomic64Load_A(top);
+  // F is guaranteed to be non-zero, since there are at least as
+  // many packets as there are waves, and each wave can hold at most
+  // one packet.
+  while (true) {
+    GLOB_ATTR header_t *P = get_header(buffer, F);
+    uint64_t N = impl::atomic64Load_X(&P->next);
+    if (impl::atomic64CAS_AX(top, &F, N))
+      break;
+    impl::deviceSleepHostWait();
+  }
+
+  return F;
+}
+
+/** \brief Use the first active lane to get a free packet and
+ *         broadcast to the whole wave.
+ */
+__static_inl uint64_t pop_free_stack(GLOB_ATTR buffer_t *buffer, uint32_t me,
+                                     uint32_t low) {
+  uint64_t packet_ptr = 0;
+  if (me == low) {
+    packet_ptr = pop(&buffer->free_stack, buffer);
+  }
+
+  uint32_t ptr_lo = packet_ptr;
+  uint32_t ptr_hi = packet_ptr >> 32;
+  ptr_lo = impl::first_lane_id(ptr_lo);
+  ptr_hi = impl::first_lane_id(ptr_hi);
+
+  return ((uint64_t)ptr_hi << 32) | ptr_lo;
+}
+
+__static_inl void push(GLOB_ATTR uint64_t *top, uint64_t ptr,
+                       GLOB_ATTR buffer_t *buffer) {
+  uint64_t F = impl::atomic64Load_X(top);
+  GLOB_ATTR header_t *P = get_header(buffer, ptr);
+
+  while (true) {
+    P->next = F;
+    if (impl::atomic64CAS_RX(top, &F, ptr))
+      break;
+    impl::deviceSleepHostWait();
+  }
+}
+
+/** \brief Use the first active lane in a wave to submit a ready
+ *         packet and signal the host.
+ */
+__static_inl void push_ready_stack(GLOB_ATTR buffer_t *buffer, uint64_t ptr,
+                                   uint32_t me, uint32_t low) {
+  if (me == low) {
+    push(&buffer->ready_stack, ptr, buffer);
+    impl::send_signal(buffer->doorbell);
+  }
+}
+
+__static_inl uint64_t inc_ptr_tag(uint64_t ptr, uint32_t index_size) {
+  // Unit step for the tag.
+  uint64_t inc = 1UL << index_size;
+  ptr += inc;
+  // When the tag for index 0 wraps, increment the tag.
+  return ptr == 0 ? inc : ptr;
+}
+
+/** \brief Return the packet after incrementing the ABA tag
+ */
+__static_inl void return_free_packet(GLOB_ATTR buffer_t *buffer, uint64_t ptr,
+                                     uint32_t me, uint32_t low) {
+  if (me == low) {
+    ptr = inc_ptr_tag(ptr, buffer->index_size);
+    push(&buffer->free_stack, ptr, buffer);
+  }
+}
+
+void __static_inl fill_packet(GLOB_ATTR header_t *header,
+                              GLOB_ATTR payload_t *payload, uint32_t service_id,
+                              uint64_t arg0, uint64_t arg1, uint64_t arg2,
+                              uint64_t arg3, uint64_t arg4, uint64_t arg5,
+                              uint64_t arg6, uint64_t arg7, uint32_t me,
+                              uint32_t low) {
+  uint64_t active = impl::get_mask();
+  if (me == low) {
+    header->service = service_id;
+    header->activemask = active;
+    uint32_t control = set_ready_flag(0);
+    header->control = control;
+  }
+  GLOB_ATTR uint64_t *ptr = payload->slots[me];
+  ptr[0] = arg0;
+  ptr[1] = arg1;
+  ptr[2] = arg2;
+  ptr[3] = arg3;
+  ptr[4] = arg4;
+  ptr[5] = arg5;
+  ptr[6] = arg6;
+  ptr[7] = arg7;
+}
+
+//  result is 8*8=64 bytes per lane
+//  Total payload could be 64 lanes * 64 bytes = 4KB
+typedef struct hostexec_result_s {
+  uint64_t arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7;
+} hostexec_result_t;
+
+/** \brief Wait for the host response and return the first two uint64_t
+ *         entries per workitem.
+ *
+ *  After the packet is submitted in READY state, the wave spins until
+ *  the host changes the state to DONE. Each workitem reads the first
+ *  two uint64_t elements in its slot and returns this.
+ */
+__static_inl hostexec_result_t get_return_value(GLOB_ATTR header_t *header,
+                                                GLOB_ATTR payload_t *payload,
+                                                uint32_t me, uint32_t low) {
+  // The while loop needs to be executed by all active
+  // lanes. Otherwise, later reads from ptr are performed only by
+  // the first thread, while other threads reuse a value cached from
+  // previous operations. The use of readfirstlane in the while loop
+  // prevents this reordering.
+  //
+  // In the absence of the readfirstlane, only one thread has a
+  // sequenced-before relation from the atomic load on
+  // header->control to the ordinary loads on ptr. As a result, the
+  // compiler is free to reorder operations in such a way that the
+  // ordinary loads are performed only by the first thread. The use
+  // of readfirstlane provides a stronger code-motion barrier, and
+  // it effectively "spreads out" the sequenced-before relation to
+  // the ordinary stores in other threads too.
+  while (true) {
+    uint32_t ready_flag = 1;
+    if (me == low) {
+      uint32_t control =
+          impl::atomic32Load_A((GLOB_ATTR uint32_t *)&header->control);
+      ready_flag = get_ready_flag(control);
+    }
+    ready_flag = impl::first_lane_id(ready_flag);
+    if (ready_flag == 0)
+      break;
+    impl::deviceSleepHostWait();
+  }
+
+  GLOB_ATTR uint64_t *ptr = (GLOB_ATTR uint64_t *)(payload->slots + me);
+  hostexec_result_t retval;
+  retval.arg0 = *ptr++;
+  retval.arg1 = *ptr++;
+  retval.arg2 = *ptr++;
+  retval.arg3 = *ptr++;
+  retval.arg4 = *ptr++;
+  retval.arg5 = *ptr++;
+  retval.arg6 = *ptr++;
+  retval.arg7 = *ptr;
+
+  return retval;
+}
+
+#undef __static_inl
+
+uint64_t [[clang::address_space(4)]] service_thread_buf
+    [[clang::loader_uninitialized]] __attribute__((used, retain, weak,
+                                                   visibility("protected")));
+extern "C" __attribute__((noinline)) hostexec_result_t
+hostexec_invoke(const uint32_t service_id, uint64_t arg0, uint64_t arg1,
+                uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5,
+                uint64_t arg6, uint64_t arg7) {
+  impl::write_needs_host_services_symbol();
+  uint32_t me = impl::lane_id();
+  uint32_t low = impl::first_lane_id(me);
+
+  GLOB_ATTR buffer_t *buffer = (GLOB_ATTR buffer_t *)service_thread_buf;
+
+  uint64_t packet_ptr = pop_free_stack(buffer, me, low);
+  GLOB_ATTR header_t *header = get_header(buffer, packet_ptr);
+  GLOB_ATTR payload_t *payload = get_payload(buffer, packet_ptr);
+  fill_packet(header, payload, service_id, arg0, arg1, arg2, arg3, arg4, arg5,
+              arg6, arg7, me, low);
+  push_ready_stack(buffer, packet_ptr, me, low);
+  hostexec_result_t retval = get_return_value(header, payload, me, low);
+  return_free_packet(buffer, packet_ptr, me, low);
+  return retval;
+}
+
+static __attribute__((flatten, always_inline)) hostexec_result_t
+hostexec_invoke_zeros(const uint32_t id, uint64_t arg0 = 0, uint64_t arg1 = 0,
+                      uint64_t arg2 = 0, uint64_t arg3 = 0, uint64_t arg4 = 0,
+                      uint64_t arg5 = 0, uint64_t arg6 = 0, uint64_t arg7 = 0) {
+  return hostexec_invoke(id, arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7);
+}
+
+extern "C" {
+// This definition of __ockl_devmem_request and __ockl_sanitizer_report needs to
+// override the weak symbol for __ockl_devmem_request and
+// __ockl_sanitizer_report in rocm device lib ockl.bc because ockl uses
+// hostcall but OpenMP uses hostexec.
+__attribute__((noinline)) uint64_t __ockl_devmem_request(uint64_t addr,
+                                                         uint64_t size) {
+  uint64_t arg0;
+  if (size) { // allocation request
+    arg0 = size;
+    hostexec_result_t result =
+        hostexec_invoke_zeros(HOSTEXEC_SID_DEVICE_MALLOC, arg0);
+    return result.arg1;
+  } else { // free request
+    arg0 = addr;
+    hostexec_result_t result =
+        hostexec_invoke_zeros(HOSTEXEC_SID_FREE, arg0);
+    return result.arg0;
+  }
+}
+
+__attribute__((noinline)) void
+__ockl_sanitizer_report(uint64_t addr, uint64_t pc, uint64_t wgidx,
+                        uint64_t wgidy, uint64_t wgidz, uint64_t wave_id,
+                        uint64_t is_read, uint64_t access_size) {
+  hostexec_result_t result =
+      hostexec_invoke(HOSTEXEC_SID_SANITIZER, addr, pc, wgidx, wgidy,
+                      wgidz, wave_id, is_read, access_size);
+}
+
+#if 0
+void f90print_(char *s) { printf("%s\n", s); }
+void f90printi_(char *s, int *i) { printf("%s %d\n", s, *i); }
+void f90printl_(char *s, long *i) { printf("%s %ld\n", s, *i); }
+void f90printf_(char *s, float *f) { printf("%s %f\n", s, *f); }
+void f90printd_(char *s, double *d) { printf("%s %g\n", s, *d); }
+uint64_t __tgt_fort_ptr_assn_i8(void *varg0, void *varg1, void *varg2,
+                                void *varg3, void *varg4) {
+  uint64_t arg0, arg1, arg2, arg3, arg4;
+  arg0 = (uint64_t)varg0;
+  arg1 = (uint64_t)varg1;
+  arg2 = (uint64_t)varg2;
+  arg3 = (uint64_t)varg3;
+  arg4 = (uint64_t)varg4;
+  hostexec_result_t result = hostexec_invoke_zeros(
+      HOSTEXEC_SID_FTNASSIGN, arg0, arg1, arg2, arg3, arg4);
+  return (uint64_t)result.arg0;
+
+// FIXME: Deprecate upstream, change test cases to use malloc & free directly
+__attribute__((flatten, always_inline)) char *global_allocate(uint32_t bufsz) {
+  return (char *)__ockl_dm_alloc(bufsz);
+}
+__attribute__((flatten, always_inline)) int global_free(void *ptr) {
+  __ockl_dm_dealloc((uint64_t)ptr);
+  return 0;
+}
+#endif
+
+char *hostexec_allocate(uint32_t bufsz) {
+  uint64_t arg0 = (uint64_t)bufsz;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_HOST_MALLOC, arg0);
+  return (char *)result.arg1;
+}
+int printf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_PRINTF, arg0, arg1);
+  return (int)result.arg0;
+}
+int fprintf_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_FPRINTF, arg0, arg1);
+  return (int)result.arg0;
+}
+void hostexec_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_VOID, arg0, arg1);
+  return;
+}
+uint32_t hostexec_uint_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_UINT, arg0, arg1);
+  return (uint32_t)result.arg0;
+}
+uint64_t hostexec_uint64_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_UINT64, arg0, arg1);
+  return (uint64_t)result.arg0;
+}
+uint64_t hostexec_fortrt_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_FORTRT, arg0, arg1);
+  return (uint64_t)result.arg0;
+}
+double hostexec_double_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_DOUBLE, arg0, arg1);
+  union {
+    uint64_t val;
+    double dval;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.dval;
+}
+int hostexec_int_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_INT, arg0, arg1);
+  return (int)result.arg0;
+}
+float hostexec_float_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_FLOAT, arg0, arg1);
+  union {
+    float fval[2];
+    uint64_t val;
+  } unionarg;
+  unionarg.val = result.arg0;
+  return unionarg.fval[0];
+}
+long hostexec_long_execute(char *print_buffer, uint32_t bufsz) {
+  uint64_t arg0, arg1;
+  arg0 = (uint64_t)bufsz;
+  arg1 = (uint64_t)print_buffer;
+  hostexec_result_t result =
+      hostexec_invoke_zeros(HOSTEXEC_SID_LONG, arg0, arg1);
+  return (long)result.arg0;
+}
+
+// Keep these declarations in sync with the ones in DeviceRTL/src/State.cpp
+// See https://github.com/llvm/llvm-project/issues/63597
+__attribute__((noinline)) extern "C" uint64_t __ockl_dm_alloc(uint64_t bufsz);
+__attribute__((noinline)) extern "C" void __ockl_dm_dealloc(uint64_t ptr);
+
+// This function is used for printf arguments that are variable length strings
+// The clang compiler will generate calls to this only when a string length is
+// not a compile time constant.
+uint32_t __strlen_max(char *instr, uint32_t maxstrlen) {
+  for (uint32_t i = 0; i < maxstrlen; i++)
+    if (instr[i] == (char)0)
+      return (uint32_t)(i + 1);
+  return maxstrlen;
+}
+
+#if defined(__NVPTX__) || defined(__AMDGCN__)
+// These are function definitions of selected fortran device runtime functions
+// that will be executed on the host using hostexec. The host functions
+// that implement the service are defined in services/execute_service.cpp.
+// These functions begin with V_ because they are variadic functions.
+// Variadic functions make it easy to reconstruct the exact arguments for
+// the call to the actual fortran host runtime function.
+
+// Since these variadic functions are static functions in execute_service.cpp,
+// we only send the index from the enum set of known functions as a fake
+// function pointer in the first arg. See the switch(DeviceRuntime_idx) in
+// execute_service.cpp which determins the correct function pointer to
+// the V_ function based on this index.
+//
+// Note that eventually these device fortran runtime functions definitions
+// will be moved to flang/runtime under the same #if above as the
+// device alternative to the host version of the functions. This will
+// require hostexec to be available.
+
+uint32_t omp_get_thread_num();
+uint32_t omp_get_num_threads();
+uint32_t omp_get_team_num();
+uint32_t omp_get_num_teams();
+
+
+#define _EXTRA_ARGS                                                            \
+  omp_get_thread_num(), omp_get_num_threads(), omp_get_team_num(),             \
+      omp_get_num_teams()
+void *_FortranAioBeginExternalListOutput(uint32_t a1, const char *a2,
+                                         uint32_t a3) {
+  void *cookie = (void *)hostexec_fortrt(
+      (void *)_FortranAioBeginExternalListOutput_idx, _EXTRA_ARGS, a1, a2, a3);
+  return cookie;
+}
+bool _FortranAioOutputAscii(void *a1, char *a2, uint64_t a3) {
+  // TODO: must use string length from a3 arg
+  a2[a3 - 1] = (char)0; // loose a char, till we create hostexec_charlen_arg
+                        // that gets length from subsequent arg
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputAscii_idx, _EXTRA_ARGS,
+                               a1, a2, a3);
+}
+bool _FortranAioOutputInteger32(void *a1, uint32_t a2) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputInteger32_idx,
+                               _EXTRA_ARGS, a1, a2);
+}
+uint32_t _FortranAioEndIoStatement(void *a1) {
+  return (uint32_t)hostexec_fortrt((void *)_FortranAioEndIoStatement_idx,
+                                   _EXTRA_ARGS, a1);
+}
+bool _FortranAioOutputInteger8(void *cookie, int8_t n) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputInteger8_idx,
+                               _EXTRA_ARGS, cookie, n);
+}
+bool _FortranAioOutputInteger16(void *cookie, int16_t n) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputInteger16_idx,
+                               _EXTRA_ARGS, cookie, n);
+}
+bool _FortranAioOutputInteger64(void *cookie, int64_t n) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputInteger64_idx,
+                               _EXTRA_ARGS, cookie, n);
+}
+bool _FortranAioOutputReal32(void *cookie, float x) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputReal32_idx, _EXTRA_ARGS,
+                               cookie, x);
+}
+bool _FortranAioOutputReal64(void *cookie, double x) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputReal64_idx, _EXTRA_ARGS,
+                               cookie, x);
+}
+bool _FortranAioOutputComplex32(void *cookie, float re, float im) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputComplex32_idx,
+                               _EXTRA_ARGS, cookie, re, im);
+}
+bool _FortranAioOutputComplex64(void *cookie, double re, double im) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputComplex64_idx,
+                               _EXTRA_ARGS, cookie, re, im);
+}
+bool _FortranAioOutputLogical(void *cookie, bool barg) {
+  return (bool)hostexec_fortrt((void *)_FortranAioOutputLogical_idx,
+                               _EXTRA_ARGS, cookie, barg);
+}
+void _FortranAAbort() {
+  hostexec_fortrt((void *)_FortranAAbort_idx);
+  // When  host service _FortranAAbort finishes, we must die from the device.
+  __builtin_trap();
+}
+void _FortranAStopStatementText(char *errmsg, int64_t a1, bool a2, bool a3) {
+  // TODO: must use string length from a1 arg
+  errmsg[a1 - 1] = (char)0;
+  hostexec_fortrt((void *)_FortranAStopStatementText_idx, errmsg, a1, a2, a3);
+  __builtin_trap();
+}
+#endif // defined(__NVPTX__) || defined(__AMDGCN__)
+
+} // end extern "C"
+#pragma omp end declare target
+#undef _EXTRA_ARGS
diff -Naur -x .git llvm-project.atd/offload/DeviceRTL/src/oclAtomics.cl llvm-project/offload/DeviceRTL/src/oclAtomics.cl
--- llvm-project.atd/offload/DeviceRTL/src/oclAtomics.cl	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/DeviceRTL/src/oclAtomics.cl	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,93 @@
+
+#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
+#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable
+
+#define __atomic_ulong atomic_ulong
+#define __inl __attribute__((flatten, always_inline))
+
+// mem order codes: A=acquire, X=relaxed, R=release
+extern __inl ulong oclAtomic64Load_A(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern __inl ulong oclAtomic64Load_X(__global __atomic_ulong * Address){
+  return  __opencl_atomic_load(Address, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern __inl uint oclAtomic32Load_A(__global const atomic_uint * Address){
+  return  __opencl_atomic_load(Address, memory_order_acquire, memory_scope_all_svm_devices);
+}
+extern __inl uint oclAtomic64CAS_AX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong( Address, e_val, new_val,
+     memory_order_acquire, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+extern __inl uint oclAtomic64CAS_RX(__global __atomic_ulong * Address,  ulong * e_val, ulong new_val) {
+   return __opencl_atomic_compare_exchange_strong(Address, e_val, new_val,
+     memory_order_release, memory_order_relaxed, memory_scope_all_svm_devices);
+}
+
+// Code for ocl_hsa_signal_add is from __ockl_hsa_signal_add because ockl
+// is not available to trunk. 
+
+typedef struct {
+  ulong handle;
+} hsa_signal_t;
+
+// from ockl
+typedef enum __ockl_memory_order_e {
+  __ockl_memory_order_relaxed = __ATOMIC_RELAXED,
+  __ockl_memory_order_acquire = __ATOMIC_ACQUIRE,
+  __ockl_memory_order_release = __ATOMIC_RELEASE,
+  __ockl_memory_order_acq_rel = __ATOMIC_ACQ_REL,
+  __ockl_memory_order_seq_cst = __ATOMIC_SEQ_CST,
+} __ockl_memory_order;
+
+// AMD Signal Kind Enumeration Values.
+typedef long amd_signal_kind64_t;
+enum amd_signal_kind_t {
+  AMD_SIGNAL_KIND_INVALID = 0,
+  AMD_SIGNAL_KIND_USER = 1,
+  AMD_SIGNAL_KIND_DOORBELL = -1,
+  AMD_SIGNAL_KIND_LEGACY_DOORBELL = -2
+};
+
+// AMD Signal.
+#define __ALIGNED__(x) __attribute__((aligned(x)))
+#define AMD_SIGNAL_ALIGN_BYTES 64
+#define AMD_SIGNAL_ALIGN __ALIGNED__(AMD_SIGNAL_ALIGN_BYTES)
+typedef struct AMD_SIGNAL_ALIGN amd_signal_s {
+  amd_signal_kind64_t kind;
+  union {
+    volatile long value;
+    __global volatile uint* legacy_hardware_doorbell_ptr;
+    __global volatile ulong* hardware_doorbell_ptr;
+  };
+  ulong event_mailbox_ptr;
+  uint event_id;
+  uint reserved1;
+  ulong start_ts;
+  ulong end_ts;
+// union {
+//   ;; __global amd_queue_t* queue_ptr;
+    ulong reserved2;
+//  };
+  uint reserved3[2];
+} amd_signal_t;
+
+#define AS(P,V,O,S) __opencl_atomic_store(P,V,O,S)
+#define AF(T,K,P,V,O,S) __opencl_atomic_fetch_##K(P,V,O,S)
+static void update_mbox(const __global amd_signal_t *sig) {
+    __global atomic_ulong *mb = (__global atomic_ulong *)sig->event_mailbox_ptr;
+    if (mb) {
+        uint id = sig->event_id;
+        AS(mb, id, memory_order_release, memory_scope_all_svm_devices);
+        __builtin_amdgcn_s_sendmsg(1 | (0 << 4), __builtin_amdgcn_readfirstlane(id) & 0xff);
+    }
+}
+extern __inl void ocl_hsa_signal_add(hsa_signal_t sig, long value,
+               __ockl_memory_order mem_order) {
+    __global amd_signal_t *s = (__global amd_signal_t *)sig.handle;
+    AF(long, add, (__global atomic_long *)&s->value, value, mem_order, memory_scope_all_svm_devices);
+    update_mbox(s);
+}
+
+#undef __atomic_ulong
+#undef __inl
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/CMakeLists.txt llvm-project/offload/plugins-nextgen/amdgpu/CMakeLists.txt
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/CMakeLists.txt	2024-09-25 10:05:15.710429955 -0500
+++ llvm-project/offload/plugins-nextgen/amdgpu/CMakeLists.txt	2024-09-25 08:28:31.616214109 -0500
@@ -4,13 +4,19 @@
 # Create the library and add the default arguments.
 add_target_library(omptarget.rtl.amdgpu AMDGPU)
 
-target_sources(omptarget.rtl.amdgpu PRIVATE src/rtl.cpp)
+target_sources(omptarget.rtl.amdgpu PRIVATE src/rtl.cpp
+        hostexec/amdgcn_hostexec.cpp
+        hostexec/amdgcn_urilocator.cpp
+        hostexec/devsanitizer.cpp
+        hostexec/execute_service.cpp)
 target_include_directories(omptarget.rtl.amdgpu PRIVATE
                            ${CMAKE_CURRENT_SOURCE_DIR}/utils)
 
-if(hsa-runtime64_FOUND AND NOT "amdgpu" IN_LIST LIBOMPTARGET_DLOPEN_PLUGINS)
+#if(hsa-runtime64_FOUND AND NOT "amdgpu" IN_LIST LIBOMPTARGET_DLOPEN_PLUGINS)
+if(hsa-runtime64_FOUND)
   message(STATUS "Building AMDGPU plugin linked against libhsa")
   target_link_libraries(omptarget.rtl.amdgpu PRIVATE hsa-runtime64::hsa-runtime64 LLVMFrontendOffloading)
+
 else()
   message(STATUS "Building AMDGPU plugin for dlopened libhsa")
   target_include_directories(omptarget.rtl.amdgpu PRIVATE dynamic_hsa)
@@ -18,6 +24,15 @@
   target_link_libraries(omptarget.rtl.amdgpu PRIVATE LLVMFrontendOffloading)
 endif()
 
+option(LIBOMPTARGET_BUILD_DEVICE_FORTRT "Build device fortran IO runtime with hostexec" ON)
+if (LIBOMPTARGET_BUILD_DEVICE_FORTRT)
+  add_compile_definitions(_HOSTEXEC_BUILD_DEVICE_FORTRT)
+  target_link_libraries(omptarget.rtl.amdgpu PRIVATE FortranDecimal
+    -L${CMAKE_BINARY_DIR}/../../lib  -L${CMAKE_INSTALL_PREFIX}/lib)
+  target_link_libraries(omptarget.rtl.amdgpu PRIVATE FortranRuntime
+    -L${CMAKE_BINARY_DIR}/../../lib  -L${CMAKE_INSTALL_PREFIX}/lib)
+endif()
+
 # Configure testing for the AMDGPU plugin. We will build tests if we could a
 # functional AMD GPU on the system, or if manually specifies by the user.
 option(LIBOMPTARGET_FORCE_AMDGPU_TESTS "Build AMDGPU libomptarget tests" OFF)
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_hostexec.cpp llvm-project/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_hostexec.cpp
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_hostexec.cpp	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_hostexec.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,513 @@
+//===---- amdgcn_hostrpc.cpp - Services thread management  ----------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This contains code for the services thread for the hostrpc system using
+// amdgcn hsa. This nvptx cuda variant of this process is in development.
+//
+//===----------------------------------------------------------------------===//
+
+// #include "hostexec_internal.h"
+#include "../../../DeviceRTL/include/Hostexec.h"
+#include "execute_service.h"
+#include "urilocator.h"
+#include <cassert>
+#include <atomic>
+#include <cstring>
+#include <functional>
+#include <iostream>
+#include <list>
+#include <mutex>
+#include <thread>
+
+#if __has_include("hsa/hsa.h")
+#include "hsa/hsa.h"
+#include "hsa/hsa_ext_amd.h"
+#else
+#include "hsa.h"
+#include "hsa_ext_amd.h"
+#endif
+
+/// Defines how many GPUs are maximally supported on a system
+#define AMD_MAX_HSA_AGENTS 256
+
+/** Opaque wrapper for signal */
+typedef struct {
+  uint64_t handle;
+} signal_t;
+
+/** Field offsets in the packet control field */
+typedef enum {
+  CONTROL_OFFSET_READY_FLAG = 0,
+  CONTROL_OFFSET_RESERVED0 = 1,
+} control_offset_t;
+
+/** Field widths in the packet control field */
+typedef enum {
+  CONTROL_WIDTH_READY_FLAG = 1,
+  CONTROL_WIDTH_RESERVED0 = 31,
+} control_width_t;
+
+/** Packet header */
+typedef struct {
+  /** Tagged pointer to the next packet in an intrusive stack */
+  uint64_t next;
+  /** Bitmask that represents payload slots with valid data */
+  uint64_t activemask;
+  /** Service ID requested by the wave */
+  uint32_t service;
+  /** Control bits.
+   *  \li \c READY flag is bit 0. Indicates packet awaiting a host response.
+   */
+  uint32_t control;
+} header_t;
+
+/** \brief Hostcall state.
+ *
+ *  Holds the state of hostcalls being requested by all kernels that
+ *  share the same hostcall state. There is usually one buffer per
+ *  device queue.
+ */
+typedef struct {
+  /** Array of 2^index_size packet headers */
+  header_t *headers;
+  /** Array of 2^index_size packet payloads */
+  payload_t *payloads;
+  /** Signal used by kernels to indicate new work */
+  signal_t doorbell;
+  /** Stack of free packets */
+  uint64_t free_stack;
+  /** Stack of ready packets */
+  uint64_t ready_stack;
+  /** Number of LSBs in the tagged pointer can index into the packet arrays */
+  uint32_t index_size;
+  /** Device ID */
+  uint32_t device_id;
+} buffer_t;
+
+enum { SIGNAL_INIT = UINT64_MAX, SIGNAL_DONE = UINT64_MAX - 1 };
+
+static uint32_t get_buffer_alignment() { return alignof(payload_t); }
+
+static uint32_t set_control_field(uint32_t control, uint8_t offset,
+                                  uint8_t width, uint32_t value) {
+  uint32_t mask = ~(((1 << width) - 1) << offset);
+  control &= mask;
+  return control | (value << offset);
+}
+
+static uint32_t reset_ready_flag(uint32_t control) {
+  return set_control_field(control, CONTROL_OFFSET_READY_FLAG,
+                           CONTROL_WIDTH_READY_FLAG, 0);
+}
+
+static uint64_t get_ptr_index(uint64_t ptr, uint32_t index_size) {
+  return ptr & ((1UL << index_size) - 1);
+}
+
+static uintptr_t align_to(uintptr_t value, uint32_t alignment) {
+  if (value % alignment == 0)
+    return value;
+  return value - (value % alignment) + alignment;
+}
+
+static uintptr_t get_header_start() {
+  return align_to(sizeof(buffer_t), alignof(header_t));
+}
+
+static uintptr_t get_payload_start(uint32_t num_packets) {
+  auto header_start = get_header_start();
+  auto header_end = header_start + sizeof(header_t) * num_packets;
+  return align_to(header_end, alignof(payload_t));
+}
+
+static size_t get_buffer_size(uint32_t num_packets) {
+  size_t buffer_size = get_payload_start(num_packets);
+  buffer_size += num_packets * sizeof(payload_t);
+  return buffer_size;
+}
+static uint64_t grab_ready_stack(buffer_t *buffer) {
+  return __atomic_exchange_n(&buffer->ready_stack, 0,
+                             std::memory_order_acquire);
+}
+static header_t *get_header(buffer_t *buffer, ulong ptr) {
+  return buffer->headers + get_ptr_index(ptr, buffer->index_size);
+}
+static payload_t *get_payload(buffer_t *buffer, ulong ptr) {
+  return buffer->payloads + get_ptr_index(ptr, buffer->index_size);
+}
+
+static signal_t create_signal() {
+  hsa_signal_t hs;
+  hsa_status_t status = hsa_signal_create(SIGNAL_INIT, 0, NULL, &hs);
+  if (status != HSA_STATUS_SUCCESS)
+    return {0};
+  return {hs.handle};
+}
+
+static hsa_amd_memory_pool_t static_host_memory_pool;
+static hsa_amd_memory_pool_t static_device_memory_pools[AMD_MAX_HSA_AGENTS];
+static hsa_agent_t static_hsa_agents[AMD_MAX_HSA_AGENTS];
+
+void save_hsa_statics(uint32_t device_id, hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool,
+                      hsa_agent_t hsa_agent) {
+  assert(device_id < AMD_MAX_HSA_AGENTS && "Supports up n GPUs");
+  static_host_memory_pool = HostMemoryPool;
+  static_device_memory_pools[device_id] = DevMemoryPool;
+  static_hsa_agents[device_id] = hsa_agent;
+}
+
+// ====== START of helper functions for execute_service ======
+service_rc host_device_mem_free(void *ptr) {
+  hsa_status_t err = hsa_amd_memory_pool_free(ptr);
+  if (err == HSA_STATUS_SUCCESS)
+    return _RC_SUCCESS;
+  else
+    return _RC_ERROR_MEMFREE;
+}
+
+service_rc host_malloc(void **ptr, size_t size, uint32_t devid) {
+  hsa_amd_memory_pool_t MemoryPool = static_host_memory_pool;
+  hsa_agent_t agent = static_hsa_agents[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, ptr);
+  if (err == HSA_STATUS_SUCCESS)
+    err = hsa_amd_agents_allow_access(1, &agent, NULL, *ptr);
+  if (err != HSA_STATUS_SUCCESS)
+    thread_abort(_RC_ERROR_HSAFAIL);
+  return _RC_SUCCESS;
+}
+
+service_rc device_malloc(void **mem, size_t size, uint32_t devid) {
+  hsa_amd_memory_pool_t MemoryPool = static_device_memory_pools[devid];
+  hsa_status_t err = hsa_amd_memory_pool_allocate(MemoryPool, size, 0, mem);
+  if (err != HSA_STATUS_SUCCESS)
+    thread_abort(_RC_ERROR_HSAFAIL);
+  return _RC_SUCCESS;
+}
+
+void thread_abort(service_rc rc) {
+  fprintf(stderr, "hostrpc thread_abort called with code %d\n", rc);
+  abort();
+}
+// ====== END helper functions for execute_service ======
+
+/** \brief Locked reference to critical data.
+ *
+ *         Simpler version of the LockedAccessor in HIP sources.
+ *
+ *         Protects access to the member _data with a lock acquired on
+ *         contruction/destruction. T must contain a _mutex field
+ *         which meets the BasicLockable requirements (lock/unlock)
+ */
+template <typename T> struct locked_accessor_t {
+  locked_accessor_t(T &criticalData) : _criticalData(&criticalData) {
+    _criticalData->_mutex.lock();
+  };
+  ~locked_accessor_t() { _criticalData->_mutex.unlock(); }
+  // Syntactic sugar so -> can be used to get the underlying type.
+  T *operator->() { return _criticalData; };
+
+private:
+  T *_criticalData;
+};
+struct record_t {
+  bool discarded;
+};
+struct critical_data_t {
+  std::unordered_map<buffer_t *, record_t> buffers;
+  std::mutex _mutex;
+};
+typedef locked_accessor_t<critical_data_t> locked_critical_data_t;
+
+typedef struct {
+  hsa_queue_t *hsa_q;
+  buffer_t *hcb;
+  uint32_t devid;
+} hsaq_buf_entry_t;
+
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+                                          uint64_t activemask,
+                                          uint32_t gpu_device,
+                                          UriLocator *uri_locator);
+struct consumer_t {
+private:
+  signal_t doorbell;
+  std::thread thread;
+  critical_data_t critical_data;
+  UriLocator *urilocator;
+  consumer_t(signal_t _doorbell) : doorbell(_doorbell) {}
+  // Table of hsa_q's and their associated buffer_t's
+  std::list<hsaq_buf_entry_t *> hsaq_bufs;
+
+public:
+  static consumer_t *create_consumer();
+
+  hsaq_buf_entry_t *add_hsaq_buf_entry(buffer_t *hcb, hsa_queue_t *hsa_q,
+                                       uint32_t devid) {
+    hsaq_buf_entry_t *new_hsaq_buf = new hsaq_buf_entry_t;
+    new_hsaq_buf->hcb = hcb;
+    new_hsaq_buf->devid = devid;
+    new_hsaq_buf->hsa_q = hsa_q;
+    hsaq_bufs.push_back(new_hsaq_buf);
+    return new_hsaq_buf;
+  }
+
+  hsaq_buf_entry_t *find_hsaq_buf_entry(hsa_queue_t *hsa_q) {
+    for (auto hsaq_buf : hsaq_bufs) {
+      if (hsaq_buf->hsa_q == hsa_q)
+        return hsaq_buf;
+    }
+    return NULL;
+  }
+
+  void process_packets(buffer_t *buffer, uint64_t ready_stack) const {
+    // This function is always called from consume_packets, which owns
+    // the lock for the critical data.
+
+    // Each wave can submit at most one packet at a time, and all
+    // waves independently push ready packets. The stack of packets at
+    // this point cannot contain multiple packets from the same wave,
+    // so consuming ready packets in a latest-first order does not
+    // affect any wave.
+    for (decltype(ready_stack) iter = ready_stack, next = 0; iter;
+         iter = next) {
+
+      // Remember the next packet pointer. The current packet will
+      // get reused from the free stack after we process it.
+      auto header = get_header(buffer, iter);
+      next = header->next;
+
+      auto payload = get_payload(buffer, iter);
+      uint64_t activemask = header->activemask;
+
+      uint service_id = header->service;
+
+      if (service_id == HOSTEXEC_SID_SANITIZER) {
+        handler_SERVICE_SANITIZER(payload, activemask, buffer->device_id,
+                                  urilocator);
+      } else {
+        // Serialize calls to execute_service for each active lane
+        // TODO: One could use ffs to skip inactive lanes faster.
+        for (uint32_t wi = 0; wi != 64; ++wi) {
+          uint64_t flag = activemask & ((uint64_t)1 << wi);
+          if (flag == 0)
+            continue;
+          execute_service(service_id, buffer->device_id, payload->slots[wi]);
+        }
+      }
+      __atomic_store_n(&header->control, reset_ready_flag(header->control),
+                       std::memory_order_release);
+    }
+  }
+
+  // FIXME: This cannot be const because it locks critical data.
+  // A lock-free implementaiton might make that possible.
+  void consume_packets() {
+    /* TODO: The consumer iterates over all registered buffers in an
+       unspecified order, and for each buffer, processes packets also
+       in an unspecified order. This may need a more efficient
+       strategy based on the turnaround time for the services
+       requested by all these packets.
+     */
+    uint64_t signal_value = SIGNAL_INIT;
+    uint64_t timeout = 1024 * 1024;
+
+    while (true) {
+      hsa_signal_t hs{doorbell.handle};
+      signal_value =
+          hsa_signal_wait_scacquire(hs, HSA_SIGNAL_CONDITION_NE, signal_value,
+                                    timeout, HSA_WAIT_STATE_BLOCKED);
+      if (signal_value == SIGNAL_DONE) {
+        return;
+      }
+
+      locked_critical_data_t data(critical_data);
+
+      for (auto ii = data->buffers.begin(), ie = data->buffers.end(); ii != ie;
+           /* don't increment here */) {
+        auto record = ii->second;
+        if (record.discarded) {
+          ii = data->buffers.erase(ii);
+          continue;
+        }
+
+        buffer_t *buffer = ii->first;
+        uint64_t F = grab_ready_stack(buffer);
+        if (F)
+          process_packets(buffer, F);
+        ++ii;
+      }
+    }
+    return;
+  }
+
+  service_rc launch_service_thread() {
+    if (thread.joinable())
+      return _RC_ERROR_CONSUMER_ACTIVE;
+    thread = std::thread(&consumer_t::consume_packets, this);
+    if (!thread.joinable())
+      return _RC_ERROR_CONSUMER_LAUNCH_FAILED;
+    return _RC_SUCCESS;
+  }
+
+  service_rc terminate_service_thread() {
+    if (!thread.joinable())
+      return _RC_ERROR_CONSUMER_INACTIVE;
+    hsa_signal_t signal = {doorbell.handle};
+    hsa_signal_store_screlease(signal, SIGNAL_DONE);
+    thread.join();
+    return _RC_SUCCESS;
+  }
+
+  void register_buffer(void *b) {
+    locked_critical_data_t data(critical_data);
+    auto buffer = reinterpret_cast<buffer_t *>(b);
+    auto &record = data->buffers[buffer];
+    record.discarded = false;
+    buffer->doorbell = doorbell;
+    urilocator = new UriLocator();
+  }
+
+  service_rc deregister_buffer(void *b) {
+    locked_critical_data_t data(critical_data);
+    auto buffer = reinterpret_cast<buffer_t *>(b);
+    if (data->buffers.count(buffer) == 0)
+      return _RC_ERROR_INVALID_REQUEST;
+    auto &record = data->buffers[buffer];
+    if (record.discarded)
+      return _RC_ERROR_INVALID_REQUEST;
+    record.discarded = true;
+    return _RC_SUCCESS;
+  }
+
+  // destructor triggered by delete static_consumer_ptr in hostrpc_terminate().
+  ~consumer_t() {
+    for (auto hsaq_buf : hsaq_bufs) {
+      if (hsaq_buf) {
+        deregister_buffer(hsaq_buf->hcb);
+        delete hsaq_buf;
+      }
+    }
+    hsaq_bufs.clear();
+    terminate_service_thread();
+    delete urilocator;
+    critical_data.buffers.clear();
+    hsa_signal_t hs{doorbell.handle};
+    hsa_signal_destroy(hs);
+  }
+
+  buffer_t *create_buffer_t(uint32_t num_packets, uint32_t devid) {
+    if (num_packets == 0) {
+      fprintf(stderr, "hostrpc create_buffer-t num_packets cannot be zero.\n");
+      thread_abort(_RC_ERROR_ZEROPACKETS);
+    }
+    size_t size = get_buffer_size(num_packets);
+    uint32_t align = get_buffer_alignment();
+    void *newbuffer = NULL;
+    service_rc err = host_malloc(&newbuffer, size + align, devid);
+    if (!newbuffer || (err != _RC_SUCCESS)) {
+      fprintf(stderr, "hostrpc call to host_malloc failed \n");
+      thread_abort(err);
+    }
+
+    if ((uintptr_t)newbuffer % get_buffer_alignment() != 0) {
+      fprintf(stderr, "ERROR: incorrect alignment \n");
+      thread_abort(_RC_ERROR_ALIGNMENT);
+    }
+
+    //  Initialize the buffer_t
+    buffer_t *hb = (buffer_t *)newbuffer;
+
+    hb->headers = (header_t *)((uint8_t *)hb + get_header_start());
+    hb->payloads =
+        (payload_t *)((uint8_t *)hb + get_payload_start(num_packets));
+
+    uint32_t index_size = 1;
+    if (num_packets > 2)
+      index_size = 32 - __builtin_clz(num_packets);
+    hb->index_size = index_size;
+    hb->headers[0].next = 0;
+
+    uint64_t next = 1UL << index_size;
+    for (uint32_t ii = 1; ii != num_packets; ++ii) {
+      hb->headers[ii].next = next;
+      next = ii;
+    }
+    hb->free_stack = next;
+    hb->ready_stack = 0;
+    hb->device_id = devid;
+    return hb;
+  }
+
+}; // end of class/struct consumer_t
+
+consumer_t *consumer_t::create_consumer() {
+  signal_t doorbell = create_signal();
+  if (doorbell.handle == 0) {
+    return nullptr;
+  }
+  return new consumer_t(doorbell);
+}
+
+// Currently, a single instance of consumer_t is created and saved statically.
+// This instance starts a single service thread for ALL devices.
+static consumer_t *static_consumer_ptr = NULL;
+
+// This is the main hostrpc function called by the amdgpu plugin when
+// launching a kernel on a designated hsa_queue_t. This function should only
+// be called if any kernel in the device image requires hostrpc services.
+extern "C" unsigned long
+hostrpc_assign_buffer(hsa_agent_t agent, hsa_queue_t *this_Q,
+                      uint32_t device_id, hsa_amd_memory_pool_t HostMemoryPool,
+                      hsa_amd_memory_pool_t DevMemoryPool) {
+  // Create and launch the services thread
+  if (!static_consumer_ptr) {
+    static_consumer_ptr = consumer_t::create_consumer();
+    service_rc err = static_consumer_ptr->launch_service_thread();
+    if (err != _RC_SUCCESS)
+      thread_abort(err);
+  }
+  // quick return to kernel launch if this hsa q is being reused
+  hsaq_buf_entry_t *hsaq_buf = static_consumer_ptr->find_hsaq_buf_entry(this_Q);
+  if (hsaq_buf)
+    return (unsigned long)hsaq_buf->hcb;
+
+  // Helper functions for execute_service need these hsa values saved
+  save_hsa_statics(device_id, HostMemoryPool, DevMemoryPool, agent);
+
+  // Get values needed to determine buffer size
+  uint32_t numCu;
+  hsa_agent_get_info(
+      agent, (hsa_agent_info_t)HSA_AMD_AGENT_INFO_COMPUTE_UNIT_COUNT, &numCu);
+  // ErrorCheck(Could not get number of cus, err);
+  uint32_t waverPerCu;
+  hsa_agent_get_info(agent,
+                     (hsa_agent_info_t)HSA_AMD_AGENT_INFO_MAX_WAVES_PER_CU,
+                     &waverPerCu);
+  // ErrorCheck(Could not get number of waves per cu, err);
+  unsigned int minpackets = numCu * waverPerCu;
+
+  //  Create and initialize the new buffer to return to kernel launch
+  buffer_t *hcb = static_consumer_ptr->create_buffer_t(minpackets, device_id);
+
+  // Register the buffer for the consumer thread
+  static_consumer_ptr->register_buffer(hcb);
+
+  // Cache in hsaq_bufs for reuse
+  hsaq_buf = static_consumer_ptr->add_hsaq_buf_entry(hcb, this_Q, device_id);
+  return (unsigned long)hcb;
+}
+
+extern "C" hsa_status_t hostrpc_terminate() {
+  if (static_consumer_ptr) {
+    // The consumer_t destructor takes care of all memory returns
+    delete static_consumer_ptr;
+    static_consumer_ptr = NULL;
+  }
+  return HSA_STATUS_SUCCESS;
+}
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_urilocator.cpp llvm-project/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_urilocator.cpp
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_urilocator.cpp	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/amdgcn_urilocator.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,228 @@
+//===---- amdgcn_urilocator.cpp - services support for urilocator  --------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This contains code to support hsa UriLocator in hostrpc.
+//
+//===----------------------------------------------------------------------===//
+
+/* Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "urilocator.h"
+#include <cstdlib>
+#include <fcntl.h>
+#include <sstream>
+#include <sys/stat.h>
+#include <unistd.h>
+
+static bool GetFileHandle(const char *fname, int *fd_ptr, size_t *sz_ptr) {
+  if ((fd_ptr == nullptr) || (sz_ptr == nullptr)) {
+    return false;
+  }
+
+  // open system function call, return false on fail
+  struct stat stat_buf;
+  *fd_ptr = open(fname, O_RDONLY);
+  if (*fd_ptr < 0) {
+    return false;
+  }
+
+  // Retrieve stat info and size
+  if (fstat(*fd_ptr, &stat_buf) != 0) {
+    close(*fd_ptr);
+    return false;
+  }
+
+  *sz_ptr = stat_buf.st_size;
+
+  return true;
+}
+
+hsa_status_t UriLocator::createUriRangeTable() {
+
+  auto execCb = [](hsa_executable_t exec, void *data) -> hsa_status_t {
+    int execState = 0;
+    hsa_status_t status;
+    status =
+        hsa_executable_get_info(exec, HSA_EXECUTABLE_INFO_STATE, &execState);
+    if (status != HSA_STATUS_SUCCESS)
+      return status;
+    if (execState != HSA_EXECUTABLE_STATE_FROZEN)
+      return status;
+
+    auto loadedCodeObjectCb = [](hsa_executable_t exec,
+                                 hsa_loaded_code_object_t lcobj,
+                                 void *data) -> hsa_status_t {
+      hsa_status_t result;
+      uint64_t loadBAddr = 0, loadSize = 0;
+      uint32_t uriLen = 0;
+      int64_t delta = 0;
+      uint64_t *argsCb = static_cast<uint64_t *>(data);
+      hsa_ven_amd_loader_1_03_pfn_t *fnTab =
+          reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(argsCb[0]);
+      std::vector<UriRange> *rangeTab =
+          reinterpret_cast<std::vector<UriRange> *>(argsCb[1]);
+
+      if (!fnTab->hsa_ven_amd_loader_loaded_code_object_get_info)
+        return HSA_STATUS_ERROR;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_BASE,
+          (void *)&loadBAddr);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_SIZE,
+          (void *)&loadSize);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI_LENGTH,
+          (void *)&uriLen);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_LOAD_DELTA,
+          (void *)&delta);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      char *uri = new char[uriLen + 1];
+      uri[uriLen] = '\0';
+      result = fnTab->hsa_ven_amd_loader_loaded_code_object_get_info(
+          lcobj, HSA_VEN_AMD_LOADER_LOADED_CODE_OBJECT_INFO_URI, (void *)uri);
+      if (result != HSA_STATUS_SUCCESS)
+        return result;
+
+      rangeTab->push_back(UriRange{loadBAddr, loadBAddr + loadSize - 1, delta,
+                                   std::string{uri, uriLen + 1}});
+      delete[] uri;
+      return HSA_STATUS_SUCCESS;
+    };
+
+    uint64_t *args = static_cast<uint64_t *>(data);
+    hsa_ven_amd_loader_1_03_pfn_t *fnExtTab =
+        reinterpret_cast<hsa_ven_amd_loader_1_03_pfn_t *>(args[0]);
+    return fnExtTab->hsa_ven_amd_loader_executable_iterate_loaded_code_objects(
+        exec, loadedCodeObjectCb, data);
+  };
+
+  if (!fn_table_.hsa_ven_amd_loader_iterate_executables)
+    return HSA_STATUS_ERROR;
+
+  uint64_t callbackArgs[2] = {(uint64_t)&fn_table_, (uint64_t)&rangeTab_};
+  return fn_table_.hsa_ven_amd_loader_iterate_executables(execCb,
+                                                          (void *)callbackArgs);
+}
+
+// Encoding of uniform-resource-identifier(URI) is detailed in
+// https://llvm.org/docs/AMDGPUUsage.html#loaded-code-object-path-uniform-resource-identifier-uri
+// The below code currently extracts the uri of loaded code object using
+// file-uri.
+std::pair<uint64_t, uint64_t> UriLocator::decodeUriAndGetFd(UriInfo &uri,
+                                                            int *uri_fd) {
+
+  std::ostringstream ss;
+  char cur;
+  uint64_t offset = 0, size = 0;
+  if (uri.uriPath.size() == 0)
+    return {0, 0};
+  auto pos = uri.uriPath.find("//");
+  if (pos == std::string::npos || uri.uriPath.substr(0, pos) != "file:") {
+    uri.uriPath = "";
+    return {0, 0};
+  }
+  auto rspos = uri.uriPath.find('#');
+  if (rspos != std::string::npos) {
+    // parse range specifier
+    std::string offprefix = "offset=", sizeprefix = "size=";
+    auto sbeg = uri.uriPath.find('&', rspos);
+    auto offbeg = rspos + offprefix.size() + 1;
+    std::string offstr = uri.uriPath.substr(offbeg, sbeg - offbeg);
+    auto sizebeg = sbeg + sizeprefix.size() + 1;
+    std::string sizestr =
+        uri.uriPath.substr(sizebeg, uri.uriPath.size() - sizebeg);
+    offset = std::stoull(offstr, nullptr, 0);
+    size = std::stoull(sizestr, nullptr, 0);
+    rspos -= 1;
+  } else {
+    rspos = uri.uriPath.size() - 1;
+  }
+  pos += 2;
+  // decode filepath
+  for (auto i = pos; i <= rspos;) {
+    cur = uri.uriPath[i];
+    if (isalnum(cur) || cur == '/' || cur == '-' || cur == '_' || cur == '.' ||
+        cur == '~') {
+      ss << cur;
+      i++;
+    } else {
+      // characters prefix with '%' char
+      char tbits = uri.uriPath[i + 1], lbits = uri.uriPath[i + 2];
+      uint8_t t = (tbits < 58) ? (tbits - 48) : ((tbits - 65) + 10);
+      uint8_t l = (lbits < 58) ? (lbits - 48) : ((lbits - 65) + 10);
+      ss << (char)(((0b00000000 | t) << 4) | l);
+      i += 3;
+    }
+  }
+  uri.uriPath = ss.str();
+  size_t fd_size;
+  GetFileHandle(uri.uriPath.c_str(), uri_fd, &fd_size);
+  // As per URI locator syntax, range_specifier is optional
+  // if range_specifier is absent return total size of the file
+  // and set offset to begin at 0.
+  if (size == 0)
+    size = fd_size;
+  return {offset, size};
+}
+
+UriLocator::UriInfo UriLocator::lookUpUri(uint64_t device_pc) {
+  UriInfo errorstate{"", 0};
+
+  if (!init_) {
+
+    hsa_status_t result;
+    result = hsa_system_get_major_extension_table(
+        HSA_EXTENSION_AMD_LOADER, 1, sizeof(fn_table_), &fn_table_);
+    if (result != HSA_STATUS_SUCCESS)
+      return errorstate;
+    result = createUriRangeTable();
+    if (result != HSA_STATUS_SUCCESS) {
+      rangeTab_.clear();
+      return errorstate;
+    }
+    init_ = true;
+  }
+
+  for (auto &seg : rangeTab_)
+    if (seg.startAddr_ <= device_pc && device_pc <= seg.endAddr_)
+      return UriInfo{seg.Uri_.c_str(), seg.elfDelta_};
+
+  return errorstate;
+}
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/devsanitizer.cpp llvm-project/offload/plugins-nextgen/amdgpu/hostexec/devsanitizer.cpp
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/devsanitizer.cpp	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/devsanitizer.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,143 @@
+//===---- devsanitizer.cpp: Definition of handler for Sanitizer Service ---===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+/*  Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+ */
+
+#include "execute_service.h"
+#include "urilocator.h"
+#include <algorithm>
+#include <assert.h>   //to exp
+#include <inttypes.h> //to exp
+#include <string>
+#include <tuple>
+#include <vector>
+
+// Address sanitizer runtime entry-function to report the invalid device memory
+// access this will be defined in llvm-project/compiler-rt/lib/asan, and will
+// have effect only when compiler-rt is build for AMDGPU. Note: This API is
+// runtime interface of asan library and only defined for linux os.
+extern "C" void __asan_report_nonself_error(
+    uint64_t *callstack, uint32_t n_callstack, uint64_t *addr, uint32_t naddr,
+    uint64_t *entity_ids, uint32_t n_entities, bool is_write,
+    uint32_t access_size, bool is_abort, const char *name, int64_t vma_adjust,
+    int fd, uint64_t file_extent_size, uint64_t file_extent_start = 0);
+
+namespace {
+extern "C" void handler_SERVICE_SANITIZER(payload_t *packt_payload,
+                                          uint64_t activemask,
+                                          uint32_t gpu_device,
+                                          UriLocator *uri_locator) {
+  // An address results in invalid access in each active lane
+  uint64_t device_failing_addresses[64];
+  // An array of identifications of entities requesting a report.
+  // index 0       - contains device id
+  // index 1,2,3   - contains wg_idx, wg_idy, wg_idz respectively.
+  // index 4 to 67 - contains reporting wave ids in a wave-front.
+  uint64_t entity_id[68], callstack[1];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  uint32_t n_activelanes = __builtin_popcountl(activemask);
+  uint64_t access_info = 0, access_size = 0;
+  bool is_abort = true;
+#endif
+#endif
+  entity_id[0] = gpu_device;
+
+  assert(packt_payload != nullptr && "packet payload is null?");
+
+  int indx = 0, en_idx = 1;
+  bool first_workitem = false;
+  for (uint32_t wi = 0; wi != 64; ++wi) {
+    uint64_t flag = activemask & ((uint64_t)1 << wi);
+    if (flag == 0)
+      continue;
+
+    auto data_slot = packt_payload->slots[wi];
+    // encoding of packet payload arguments is
+    // defined in device-libs/asanrtl/src/report.cl
+    if (!first_workitem) {
+      device_failing_addresses[indx] = data_slot[0];
+      callstack[0] = data_slot[1];
+      entity_id[en_idx] = data_slot[2];
+      entity_id[++en_idx] = data_slot[3];
+      entity_id[++en_idx] = data_slot[4];
+      entity_id[++en_idx] = data_slot[5];
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+      access_info = data_slot[6];
+      access_size = data_slot[7];
+#endif
+#endif
+      first_workitem = true;
+    } else {
+      device_failing_addresses[indx] = data_slot[0];
+      entity_id[en_idx] = data_slot[5];
+    }
+    indx++;
+    en_idx++;
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  bool is_write = false;
+  if (access_info & 0xFFFFFFFF00000000)
+    is_abort = false;
+  if (access_info & 1)
+    is_write = true;
+#endif
+#endif
+
+  std::string fileuri;
+  uint64_t size = 0, offset = 0;
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  int64_t loadAddrAdjust = 0;
+#endif
+#endif
+  int uri_fd = -1;
+
+  if (uri_locator) {
+    UriLocator::UriInfo fileuri_info = uri_locator->lookUpUri(callstack[0]);
+    std::tie(offset, size) =
+        uri_locator->decodeUriAndGetFd(fileuri_info, &uri_fd);
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+    loadAddrAdjust = fileuri_info.loadAddressDiff;
+#endif
+#endif
+  }
+
+#if SANITIZER_AMDGPU
+#if defined(__linux__)
+  __asan_report_nonself_error(
+      callstack, 1, device_failing_addresses, n_activelanes, entity_id,
+      n_activelanes + 4, is_write, access_size, is_abort,
+      /*thread key*/ "amdgpu", loadAddrAdjust, uri_fd, size, offset);
+#endif
+#endif
+}
+} // end anonymous namespace
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/execute_service.cpp llvm-project/offload/plugins-nextgen/amdgpu/hostexec/execute_service.cpp
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/execute_service.cpp	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/execute_service.cpp	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,1390 @@
+//===---- execute_service.cpp - support for hostrpc services --------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains architecture independed host code for hostrpc services
+// Calls to hostrpc_execute are serialized by the thread and packet manager.
+// For printf and fprintf, this code reconstructs the host variable argument
+// ABI to support alls to vprintf and vfprintf.  This is facilitated by
+// a robust buffer packaging scheme defined in Clang codegen. The same buffer
+// packaging scheme is used for hostexec fuctions.  Hostexec functions support
+// the launching of host variadic functions from the GPU.
+//
+//===----------------------------------------------------------------------===//
+
+/* MIT License
+
+Copyright © 2023 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+#include "execute_service.h"
+#include "hostexec.h"
+#include "../../../DeviceRTL/include/Hostexec.h"
+#include <assert.h>
+#include <cstring>
+#include <ctype.h>
+#include <list>
+#include <stdarg.h>
+#include <stdbool.h>
+#include <stddef.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <tuple>
+#include <vector>
+
+// MAXVARGS applies to non-printf vargs functions only.
+#define MAXVARGS 32
+// NUMFPREGS and FPREGSZ are part of x86 vargs ABI that
+// is recreated with the printf support.
+#define NUMFPREGS 8
+#define FPREGSZ 16
+
+typedef int uint128_t __attribute__((mode(TI)));
+struct hostrpc_pfIntRegs {
+  uint64_t rdi, rsi, rdx, rcx, r8, r9;
+};
+typedef struct hostrpc_pfIntRegs hostrpc_pfIntRegs_t; // size = 48 bytes
+
+struct hostrpc_pfRegSaveArea {
+  hostrpc_pfIntRegs_t iregs;
+  uint128_t freg[NUMFPREGS];
+};
+typedef struct hostrpc_pfRegSaveArea
+    hostrpc_pfRegSaveArea_t; // size = 304 bytes
+
+struct hostrpc_ValistExt {
+  uint32_t gp_offset;      /* offset to next available gpr in reg_save_area */
+  uint32_t fp_offset;      /* offset to next available fpr in reg_save_area */
+  void *overflow_arg_area; /* args that are passed on the stack */
+  hostrpc_pfRegSaveArea_t *reg_save_area; /* int and fp registers */
+  size_t overflow_size;
+} __attribute__((packed));
+typedef struct hostrpc_ValistExt hostrpc_ValistExt_t;
+
+/// Prototype for host fallback functions
+// typedef uint32_t hostexec_uint_t(void *, ...);
+// typedef uint64_t hostexec_uint64_t(void *, ...);
+// typedef double   hostexec_double_t(void *, ...);
+
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint32_t *rc);
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint32_t *rc);
+
+template <typename T, typename FT>
+static service_rc hostexec_service(char *buf, size_t bufsz, T *rc);
+
+static void handler_SERVICE_PRINTF(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_printf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc rcmem = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)rcmem;
+}
+static void handler_SERVICE_FPRINTF(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  uint uint_value;
+  service_rc rc = hostrpc_fprintf(device_buffer, bufsz, &uint_value);
+  payload[0] = (uint64_t)uint_value; // what the printf returns
+  payload[1] = (uint64_t)rc;         // Any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err;
+}
+
+template <typename T, typename TF>
+static void handler_SERVICE_VARFN(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  payload[0] = 0; // zero 64 bits
+  service_rc rc = hostexec_service<T, TF>(device_buffer, bufsz, (T *)payload);
+  // payload[0] has the return value
+  // payload[1] reserved for 128 bit values such as double complex
+  // payload[2-3] reserved for 256 bit return values
+  payload[1] = (uint64_t)rc; // any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err; // any errors on memory free
+}
+
+static service_rc fortran_rt_service(char *buf, size_t bufsz, uint64_t *rc);
+
+static void handler_SERVICE_FORTRT(uint32_t device_id, uint64_t *payload) {
+  size_t bufsz = (size_t)payload[0];
+  char *device_buffer = (char *)payload[1];
+  payload[0] = 0; // zero 64 bits
+  service_rc rc = fortran_rt_service(device_buffer, bufsz, (uint64_t *)payload);
+  // payload[0] has the return value
+  // payload[1] reserved for 128 bit values such as double complex
+  // payload[2-3] reserved for 256 bit return values
+  payload[1] = (uint64_t)rc; // any errors in the service function
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[2] = (uint64_t)err; // any errors on memory free
+}
+
+static void handler_SERVICE_HOST_MALLOC(uint32_t device_id, uint64_t *payload) {
+  void *ptr = NULL;
+  // CPU device ID 0 is the fine grain memory
+  size_t sz = (size_t)payload[0];
+  service_rc err = host_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+//  SERVICE_MALLOC & SERVICE_FREE are for allocating a heap of device memory
+//  only used by the device to be used for device side malloc and free.
+//  This is called by __ockl_devmem_request. For allocating memory visible
+//  to both host and device user SERVICE_HOST_MALLOC. The corresponding
+//  vargs function will release this
+static void handler_SERVICE_MALLOC(uint32_t device_id, uint64_t *payload) {
+  void *ptr = NULL;
+  size_t sz = (size_t)payload[0];
+  service_rc err = device_malloc(&ptr, sz, device_id);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+
+#if 0
+void fort_ptr_assign_i8(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  printf("\n\n ERROR: hostrpc service FTNASSIGN is not functional\n\n");
+};
+service_rc FtnAssignWrapper(void *arg0, void *arg1, void *arg2, void *arg3, void *arg4) {
+  fort_ptr_assign_i8(arg0, arg1, arg2, arg3, arg4);
+  return HSA_STATUS_SUCCESS;
+}
+
+service_rc ftn_assign_wrapper(void *arg0, void *arg1, void *arg2, void *arg3,
+                                void *arg4) {
+  return FtnAssignWrapper(arg0, arg1, arg2, arg3, arg4);
+}
+
+static void handler_SERVICE_FTNASSIGN(uint32_t device_id,
+                                              uint64_t *payload) {
+  void *ptr = NULL;
+  service_rc err = ftn_assign_wrapper((void *)payload[0], (void *)payload[1],
+                                        (void *)payload[2], (void *)payload[3],
+                                        (void *)payload[4]);
+  payload[0] = (uint64_t)err;
+  payload[1] = (uint64_t)ptr;
+}
+#endif
+
+static void handler_SERVICE_FREE(uint32_t device_id, uint64_t *payload) {
+  char *device_buffer = (char *)payload[0];
+  service_rc err = host_device_mem_free(device_buffer);
+  payload[0] = (uint64_t)err;
+}
+
+static bool trace_init = false;
+static bool host_exec_trace;
+#define _CCHAR (const char *)
+static const char *TrcStrs[HOSTEXEC_SID_VOID + 1] = {
+    _CCHAR "unsed",       _CCHAR "terminate", _CCHAR "device_malloc",
+    _CCHAR "host_malloc", _CCHAR "free",      _CCHAR "printf",
+    _CCHAR "fprintf",     _CCHAR "ftnassign", _CCHAR "sanatizer",
+    _CCHAR "uint",        _CCHAR "uint64",    _CCHAR "double",
+    _CCHAR "int",         _CCHAR "long",      _CCHAR "float",
+    _CCHAR "void"};
+#undef _CCAR
+// The consumer thread will serialize each active lane and call execute_service
+// for the service request. These services are intended to be architecturally
+// independent.
+void execute_service(uint32_t service_id, uint32_t device_id,
+                     uint64_t *payload) {
+  if (!trace_init) {
+    trace_init = true;
+    if (char *EnvStr = getenv("LIBOMPTARGET_HOSTEXEC_TRACE"))
+      host_exec_trace = std::atoi(EnvStr) != 0;
+  }
+  if (host_exec_trace)
+    fprintf(stderr, "Hostexec service: %s SrvId: %d DevId: %d PayLoad: %lu\n",
+                    TrcStrs[service_id], service_id, device_id, payload[0]);
+
+  switch (service_id) {
+  case HOSTEXEC_SID_PRINTF:
+    handler_SERVICE_PRINTF(device_id, payload);
+    break;
+  case HOSTEXEC_SID_FPRINTF:
+    handler_SERVICE_FPRINTF(device_id, payload);
+    break;
+  case HOSTEXEC_SID_VOID:
+    // Cannot return a void in template so just use uint64_t
+    handler_SERVICE_VARFN<uint64_t, hostexec_uint64_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_UINT:
+    handler_SERVICE_VARFN<uint, hostexec_uint_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_UINT64:
+    handler_SERVICE_VARFN<uint64_t, hostexec_uint64_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_DOUBLE:
+    handler_SERVICE_VARFN<double, hostexec_double_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_INT:
+    handler_SERVICE_VARFN<int, hostexec_int_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_LONG:
+    handler_SERVICE_VARFN<long, hostexec_long_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_FLOAT:
+    handler_SERVICE_VARFN<float, hostexec_float_t>(device_id, payload);
+    break;
+  case HOSTEXEC_SID_HOST_MALLOC:
+    handler_SERVICE_HOST_MALLOC(device_id, payload);
+    break;
+  case HOSTEXEC_SID_DEVICE_MALLOC:
+    handler_SERVICE_MALLOC(device_id, payload);
+    break;
+    //  case HOSTEXEC_SID_FTNASSIGN:
+    //    handler_SERVICE_FTNASSIGN(device_id, payload);
+    //    break;
+  case HOSTEXEC_SID_FREE:
+    handler_SERVICE_FREE(device_id, payload);
+    break;
+  case HOSTEXEC_SID_FORTRT:
+    handler_SERVICE_FORTRT(device_id, payload);
+    break;
+  default:
+    fprintf(stderr, "ERROR: hostrpc got a bad service id:%d\n", service_id);
+    thread_abort(_RC_INVALIDSERVICE_ERROR);
+  }
+}
+
+// Support for hostrpc_printf service
+
+// Handle overflow when building the va_list for vprintf
+static service_rc hostrpc_pfGetOverflow(hostrpc_ValistExt_t *valist,
+                                        size_t needsize) {
+  if (needsize < valist->overflow_size)
+    return _RC_SUCCESS;
+
+  // Make the overflow area bigger
+  size_t stacksize;
+  void *newstack;
+  if (valist->overflow_size == 0) {
+    // Make initial save area big to reduce mallocs
+    stacksize = (FPREGSZ * NUMFPREGS) * 2;
+    if (needsize > stacksize)
+      stacksize = needsize; // maybe a big string
+  } else {
+    // Initial save area not big enough, double it
+    stacksize = valist->overflow_size * 2;
+  }
+  if (!(newstack = malloc(stacksize))) {
+    return _RC_STATUS_ERROR;
+  }
+  memset(newstack, 0, stacksize);
+  if (valist->overflow_size) {
+    memcpy(newstack, valist->overflow_arg_area, valist->overflow_size);
+    free(valist->overflow_arg_area);
+  }
+  valist->overflow_arg_area = newstack;
+  valist->overflow_size = stacksize;
+  return _RC_SUCCESS;
+}
+
+// Add an integer to the va_list for vprintf
+static service_rc hostrpc_pfAddInteger(hostrpc_ValistExt_t *valist, char *val,
+                                       size_t valsize, size_t *stacksize) {
+  uint64_t ival;
+  switch (valsize) {
+  case 1:
+    ival = *(uint8_t *)val;
+    break;
+  case 2:
+    ival = *(uint32_t *)val;
+    break;
+  case 4:
+    ival = (*(uint32_t *)val);
+    break;
+  case 8:
+    ival = *(uint64_t *)val;
+    break;
+  default: {
+    return _RC_STATUS_ERROR;
+  }
+  }
+  //  Always copy 8 bytes, sizeof(ival)
+  if ((valist->gp_offset + sizeof(ival)) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), &ival,
+           sizeof(ival));
+    valist->gp_offset += sizeof(ival);
+    return _RC_SUCCESS;
+  }
+  // Ensure valist overflow area is big enough
+  size_t needsize = (size_t)*stacksize + sizeof(ival);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  // Copy to overflow
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &ival,
+         sizeof(ival));
+
+  *stacksize += sizeof(ival);
+  return _RC_SUCCESS;
+}
+
+// Add a String argument when building va_list for vprintf
+static service_rc hostrpc_pfAddString(hostrpc_ValistExt_t *valist, char *val,
+                                      size_t strsz, size_t *stacksize) {
+  size_t valsize =
+      sizeof(char *); // ABI captures pointer to string,  not string
+  if ((valist->gp_offset + valsize) <= sizeof(hostrpc_pfIntRegs_t)) {
+    memcpy(((char *)valist->reg_save_area + valist->gp_offset), val, valsize);
+    valist->gp_offset += valsize;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + valsize;
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, val,
+         valsize);
+  *stacksize += valsize;
+  return _RC_SUCCESS;
+}
+
+// Add a floating point value when building va_list for vprintf
+static service_rc hostrpc_pfAddFloat(hostrpc_ValistExt_t *valist, char *numdata,
+                                     size_t valsize, size_t *stacksize) {
+  // FIXME, we can used load because doubles are now aligned
+  double dval;
+  if (valsize == 4) {
+    float fval;
+    memcpy(&fval, numdata, 4);
+    dval = (double)fval; // Extend single to double per abi
+  } else if (valsize == 8) {
+    memcpy(&dval, numdata, 8);
+  } else {
+    return _RC_STATUS_ERROR;
+  }
+  if ((valist->fp_offset + FPREGSZ) <= sizeof(hostrpc_pfRegSaveArea_t)) {
+    memcpy(((char *)valist->reg_save_area + (size_t)(valist->fp_offset)), &dval,
+           sizeof(double));
+    valist->fp_offset += FPREGSZ;
+    return _RC_SUCCESS;
+  }
+  size_t needsize = (size_t)*stacksize + sizeof(double);
+  if (hostrpc_pfGetOverflow(valist, needsize) != _RC_SUCCESS)
+    return _RC_STATUS_ERROR;
+  memcpy((char *)(valist->overflow_arg_area) + (size_t)*stacksize, &dval,
+         sizeof(double));
+  // move only by the size of the double (8 bytes)
+  *stacksize += sizeof(double);
+  return _RC_SUCCESS;
+}
+
+// We would like to get llvm typeID enum from Type.h. e.g.
+// #include "../../../../../llvm/include/llvm/IR/Type.h"
+// But we cannot include LLVM headers in a runtime function.
+// So we a have a manual copy of llvm TypeID enum from Type.h
+enum TypeID {
+  // PrimitiveTypes
+  HalfTyID = 0,  ///< 16-bit floating point type
+  BFloatTyID,    ///< 16-bit floating point type (7-bit significand)
+  FloatTyID,     ///< 32-bit floating point type
+  DoubleTyID,    ///< 64-bit floating point type
+  X86_FP80TyID,  ///< 80-bit floating point type (X87)
+  FP128TyID,     ///< 128-bit floating point type (112-bit significand)
+  PPC_FP128TyID, ///< 128-bit floating point type (two 64-bits, PowerPC)
+  VoidTyID,      ///< type with no size
+  LabelTyID,     ///< Labels
+  MetadataTyID,  ///< Metadata
+  X86_AMXTyID,   ///< AMX vectors (8192 bits, X86 specific)
+  TokenTyID,     ///< Tokens
+
+  // Derived types... see DerivedTypes.h file.
+  IntegerTyID,        ///< Arbitrary bit width integers
+  FunctionTyID,       ///< Functions
+  PointerTyID,        ///< Pointers
+  StructTyID,         ///< Structures
+  ArrayTyID,          ///< Arrays
+  FixedVectorTyID,    ///< Fixed width SIMD vector type
+  ScalableVectorTyID, ///< Scalable SIMD vector type
+  TypedPointerTyID,   ///< Typed pointer used by some GPU targets
+  TargetExtTyID,      ///< Target extension type
+};
+
+// Build an extended va_list for vprintf by unpacking the buffer
+static service_rc hostrpc_pfBuildValist(hostrpc_ValistExt_t *valist,
+                                        int NumArgs, char *keyptr,
+                                        char *dataptr, char *strptr,
+                                        size_t *data_not_used) {
+  hostrpc_pfRegSaveArea_t *regs;
+  size_t regs_size = sizeof(*regs);
+  regs = (hostrpc_pfRegSaveArea_t *)malloc(regs_size);
+  if (!regs)
+    return _RC_STATUS_ERROR;
+  memset(regs, 0, regs_size);
+  *valist = (hostrpc_ValistExt_t){
+      .gp_offset = 0,
+      .fp_offset = 0,
+      .overflow_arg_area = NULL,
+      .reg_save_area = regs,
+      .overflow_size = 0,
+  };
+
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  size_t stacksize = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (valist->fp_offset == 0)
+        valist->fp_offset = sizeof(hostrpc_pfIntRegs_t);
+      if (hostrpc_pfAddFloat(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDFLOAT_ERROR;
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+      if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+        return _RC_ADDINT_ERROR;
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddString(valist, (char *)&strptr, strsz, &stacksize))
+          return _RC_ADDSTRING_ERROR;
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        if (hostrpc_pfAddInteger(valist, dataptr, num_bytes, &stacksize))
+          return _RC_ADDINT_ERROR;
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+  }
+  return _RC_SUCCESS;
+} // end hostrpc_pfBuildValist
+
+/*
+ *  The buffer to pack arguments for all vargs functions has thes 4 sections:
+ *  1. Header        datalen 4 bytes
+ *                   numargs 4 bytes
+ *  2. Keys          A 4-byte key for each arg including string args
+ *                   Each 4-byte key contains llvmID and numbits to
+ *                   describe the datatype.
+ *  3. args_data     Ths data values for each argument.
+ *                   Each arg is aligned according to its size.
+ *                   If the field is a string
+ *                   the dataptr contains the string length.
+ *  4. strings_data  Exection time string values
+ */
+static service_rc hostrpc_fprintf(char *buf, size_t bufsz, uint *rc) {
+
+  // FIXME: Put the collection of these 6 values in a function
+  //        All service routines that use vargs will need these values.
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // Skip past the file pointer and format string argument
+  size_t fillerNeeded = ((size_t)dataptr) % 8;
+  if (fillerNeeded)
+    dataptr += fillerNeeded; // dataptr is now aligned on 8 byte
+  // Cannot convert directly to FILE*, so convert to 8-byte size_t first
+  FILE *fileptr = (FILE *)*((size_t *)dataptr);
+  dataptr += sizeof(FILE *); // skip past file ptr
+  NumArgs = NumArgs - 2;
+  keyptr += 8; // All keys are 4 bytes
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; //  for strings the data value is the size, not a key
+  char *fmtstr = strptr;
+  strptr += strsz;
+  data_not_used -= (sizeof(FILE *) + 4); // 12
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vfprintf(fileptr, fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+//  This the main service routine for printf
+static service_rc hostrpc_printf(char *buf, size_t bufsz, uint *rc) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Skip past the format string argument
+  char *fmtstr = strptr;
+  NumArgs--;
+  keyptr += 4;
+  size_t strsz = (size_t) * (unsigned int *)dataptr;
+  dataptr += 4; // for strings the data value is the size, not a real pointer
+  strptr += strsz;
+  data_not_used -= 4;
+
+  hostrpc_ValistExt_t valist;
+  va_list *real_va_list;
+  real_va_list = (va_list *)&valist;
+
+  if (hostrpc_pfBuildValist(&valist, NumArgs, keyptr, dataptr, strptr,
+                            &data_not_used) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // Roll back offsets and save stack pointer for
+  valist.gp_offset = 0;
+  valist.fp_offset = sizeof(hostrpc_pfIntRegs_t);
+  void *save_stack = valist.overflow_arg_area;
+
+  *rc = vprintf(fmtstr, *real_va_list);
+
+  if (valist.reg_save_area)
+    free(valist.reg_save_area);
+  if (save_stack)
+    free(save_stack);
+
+  return _RC_SUCCESS;
+}
+
+//---------------- Support for hostexec_* service ---------------------
+//
+
+// These are the helper functions for hostexec_<TYPE>_ functions
+static uint64_t getuint32(char *val) {
+  uint32_t i32 = *(uint32_t *)val;
+  return (uint64_t)i32;
+}
+static uint64_t getuint64(char *val) { return *(uint64_t *)val; }
+
+static void *getfnptr(char *val) {
+  uint64_t ival = *(uint64_t *)val;
+  return (void *)ival;
+}
+
+// build argument array
+static service_rc hostrpc_build_vargs_array(int NumArgs, char *keyptr,
+                                            char *dataptr, char *strptr,
+                                            size_t *data_not_used,
+                                            uint64_t *a[MAXVARGS]) {
+  size_t num_bytes;
+  size_t bytes_consumed;
+  size_t strsz;
+  size_t fillerNeeded;
+
+  uint argcount = 0;
+
+  for (int argnum = 0; argnum < NumArgs; argnum++) {
+    num_bytes = 0;
+    strsz = 0;
+    unsigned int key = *(unsigned int *)keyptr;
+    unsigned int llvmID = key >> 16;
+    unsigned int numbits = (key << 16) >> 16;
+
+    switch (llvmID) {
+    case FloatTyID:  ///<  2: 32-bit floating point type
+    case DoubleTyID: ///<  3: 64-bit floating point type
+    case FP128TyID:  ///<  5: 128-bit floating point type (112-bit mantissa)
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case IntegerTyID: ///< 11: Arbitrary bit width integers
+      num_bytes = numbits / 8;
+      bytes_consumed = num_bytes;
+      fillerNeeded = ((size_t)dataptr) % num_bytes;
+      if (fillerNeeded) {
+        dataptr += fillerNeeded;
+        bytes_consumed += fillerNeeded;
+      }
+      if ((*data_not_used) < bytes_consumed)
+        return _RC_DATA_USED_ERROR;
+
+      if (num_bytes == 4)
+        a[argcount] = (uint64_t *)getuint32(dataptr);
+      else
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+
+      break;
+
+    case PointerTyID:     ///< 15: Pointers
+      if (numbits == 1) { // This is a pointer to string
+        num_bytes = 4;
+        bytes_consumed = num_bytes;
+        strsz = (size_t) * (unsigned int *)dataptr;
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+        a[argcount] = (uint64_t *)((char *)strptr);
+
+      } else {
+        num_bytes = 8;
+        bytes_consumed = num_bytes;
+        fillerNeeded = ((size_t)dataptr) % num_bytes;
+        if (fillerNeeded) {
+          dataptr += fillerNeeded; // dataptr is now aligned
+          bytes_consumed += fillerNeeded;
+        }
+        if ((*data_not_used) < bytes_consumed)
+          return _RC_DATA_USED_ERROR;
+
+        a[argcount] = (uint64_t *)getuint64(dataptr);
+      }
+      break;
+
+    case HalfTyID:           ///<  1: 16-bit floating point type
+    case ArrayTyID:          ///< 14: Arrays
+    case StructTyID:         ///< 13: Structures
+    case FunctionTyID:       ///< 12: Functions
+    case TokenTyID:          ///< 10: Tokens
+    case MetadataTyID:       ///<  8: Metadata
+    case LabelTyID:          ///<  7: Labels
+    case PPC_FP128TyID:      ///<  6: 128-bit floating point type (two 64-bits,
+                             ///<  PowerPC)
+    case X86_FP80TyID:       ///<  4: 80-bit floating point type (X87)
+    case FixedVectorTyID:    ///< 16: Fixed width SIMD vector type
+    case ScalableVectorTyID: ///< 17: Scalable SIMD vector type
+    case TypedPointerTyID:   ///< Typed pointer used by some GPU targets
+    case TargetExtTyID:      ///< Target extension type
+    case VoidTyID:
+      return _RC_UNSUPPORTED_ID_ERROR;
+      break;
+    default:
+      return _RC_INVALID_ID_ERROR;
+    }
+
+    // Move to next argument
+    dataptr += num_bytes;
+    strptr += strsz;
+    *data_not_used -= bytes_consumed;
+    keyptr += 4;
+    argcount++;
+  }
+  return _RC_SUCCESS;
+}
+
+// Make the vargs function call to the function pointer fnptr
+// by casting fnptr to vfnptr. Return uint32_t
+template <typename T, typename FT>
+static service_rc hostrpc_call_fnptr(uint32_t NumArgs, void *fnptr,
+                                     uint64_t *a[MAXVARGS], T *rv) {
+  //
+  // Currently users are instructed that the first arg must be reserved
+  // for device side to store function pointer. Removing this requirement
+  // is much more difficult that it appears.  One change of many is to
+  // remove fnptr in the call sites below. 2nd is to change the host
+  // side macro in hostexec.h to remove the fn arg. This results in the
+  // symbol for the variadic function being undefined at GPU link time.
+  // This is because device compilation must ignore variadic function
+  // definitions.
+  //
+  // This is a major design decision which would change the test case.
+  //
+  FT *vfnptr = (FT *)fnptr;
+
+  switch (NumArgs) {
+  case 1:
+    *rv = vfnptr(fnptr, a[0]);
+    break;
+  case 2:
+    *rv = vfnptr(fnptr, a[0], a[1]);
+    break;
+  case 3:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2]);
+    break;
+  case 4:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3]);
+    break;
+  case 5:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4]);
+    break;
+  case 6:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5]);
+    break;
+  case 7:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6]);
+    break;
+  case 8:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7]);
+    break;
+  case 9:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8]);
+    break;
+  case 10:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9]);
+    break;
+  case 11:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10]);
+    break;
+  case 12:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11]);
+    break;
+  case 13:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12]);
+    break;
+  case 14:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13]);
+    break;
+  case 15:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14]);
+    break;
+  case 16:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15]);
+    break;
+  case 17:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16]);
+    break;
+  case 18:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17]);
+    break;
+  case 19:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18]);
+    break;
+  case 20:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19]);
+    break;
+  case 21:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20]);
+    break;
+  case 22:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21]);
+    break;
+  case 23:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22]);
+    break;
+  case 24:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23]);
+    break;
+  case 25:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24]);
+    break;
+  case 26:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25]);
+    break;
+  case 27:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26]);
+    break;
+  case 28:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27]);
+    break;
+  case 29:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28]);
+    break;
+  case 30:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29]);
+    break;
+  case 31:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30]);
+    break;
+  case 32:
+    *rv = vfnptr(fnptr, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7], a[8],
+                 a[9], a[10], a[11], a[12], a[13], a[14], a[15], a[16], a[17],
+                 a[18], a[19], a[20], a[21], a[22], a[23], a[24], a[25], a[26],
+                 a[27], a[28], a[29], a[30], a[31]);
+    break;
+  default:
+    return _RC_EXCEED_MAXVARGS_ERROR;
+  }
+  return _RC_SUCCESS;
+}
+
+template <typename T, typename FT>
+static service_rc hostexec_service(char *buf, size_t bufsz, T *return_value) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // skip the function pointer arg including any align buffer
+  if (((size_t)dataptr) % (size_t)8) {
+    dataptr += 4;
+    data_not_used -= 4;
+  }
+  void *fnptr = getfnptr(dataptr);
+  NumArgs--;
+  keyptr += 4;
+  dataptr += 8;
+  data_not_used -= 4;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  uint64_t *a[MAXVARGS];
+  if (hostrpc_build_vargs_array(NumArgs, keyptr, dataptr, strptr,
+                                &data_not_used, a) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  if (hostrpc_call_fnptr<T, FT>(NumArgs, fnptr, a, return_value) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  return _RC_SUCCESS;
+}
+
+// Headers for Host Fortran Runtime API as built in flang/runtime
+extern "C" { 
+extern void *_FortranAioBeginExternalListOutput(uint32_t a1, const char *a2,
+                                                uint32_t a3);
+extern bool _FortranAioOutputAscii(void *a1, char *a2, uint64_t a3);
+extern bool _FortranAioOutputInteger32(void *a1, uint32_t a2);
+extern uint32_t _FortranAioEndIoStatement(void *a1);
+extern bool _FortranAioOutputInteger8(void *cookie, __int8_t n);
+extern bool _FortranAioOutputInteger16(void *cookie, __int16_t n);
+extern bool _FortranAioOutputInteger64(void *cookie, __int64_t n);
+extern bool _FortranAioOutputReal32(void *cookie, float x);
+extern bool _FortranAioOutputReal64(void *cookie, double x);
+extern bool _FortranAioOutputComplex32(void *cookie, float re, float im);
+extern bool _FortranAioOutputComplex64(void *cookie, double re, double im);
+extern bool _FortranAioOutputLogical(void *cookie, bool truth);
+extern void _FortranAAbort();
+extern void _FortranAStopStatementText(char *errmsg, int64_t a1, bool a2,
+                                       bool a3);
+
+//  Save the cookie because deferred functions have execution reordered.
+static void *_list_started_cookie = nullptr;
+extern void *V_FortranAioBeginExternalListOutput(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  int32_t v0 = va_arg(args, int32_t);
+  const char *v1 = va_arg(args, const char *);
+  int32_t v2 = va_arg(args, int32_t);
+  va_end(args);
+  void *cookie = _FortranAioBeginExternalListOutput(v0, v1, v2);
+  _list_started_cookie = cookie;
+  return cookie;
+}
+extern bool V_FortranAioOutputAscii(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  char *v1 = va_arg(args, char *);
+  uint64_t v2 = va_arg(args, uint64_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputAscii(v0, v1, v2);
+}
+extern bool V_FortranAioOutputInteger32(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint32_t v1 = va_arg(args, uint32_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputInteger32(v0, v1);
+}
+extern uint32_t V_FortranAioEndIoStatement(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  va_end(args);
+  v0 = _list_started_cookie;
+  uint32_t rv = _FortranAioEndIoStatement(v0);
+  return rv;
+}
+extern bool V_FortranAioOutputInteger8(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint32_t v1 = va_arg(args, uint32_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputInteger8(v0, v1);
+}
+extern bool V_FortranAioOutputInteger16(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint32_t v1 = va_arg(args, uint32_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputInteger16(v0, v1);
+}
+extern bool V_FortranAioOutputInteger64(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint32_t v1 = va_arg(args, uint32_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputInteger64(v0, v1);
+}
+extern bool V_FortranAioOutputReal32(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint64_t v1 = va_arg(args, uint64_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  double dv;
+  memcpy(&dv, &v1, 8);
+  return _FortranAioOutputReal32(v0, (float)dv);
+}
+extern bool V_FortranAioOutputReal64(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *cookie = va_arg(args, void *);
+  uint64_t v1 = va_arg(args, uint64_t);
+  va_end(args);
+  cookie = _list_started_cookie;
+  double dv;
+  memcpy(&dv, &v1, 8);
+  return _FortranAioOutputReal64(cookie, dv);
+}
+extern bool V_FortranAioOutputComplex32(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint64_t v1 = va_arg(args, uint64_t);
+  uint64_t v2 = va_arg(args, uint64_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  double dv1, dv2;
+  memcpy(&dv1, &v1, 8);
+  memcpy(&dv2, &v2, 8);
+  return _FortranAioOutputComplex32(v0, (float)dv1, (float)dv2);
+}
+extern bool V_FortranAioOutputComplex64(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint64_t v1 = va_arg(args, uint64_t);
+  uint64_t v2 = va_arg(args, uint64_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  double dv1, dv2;
+  memcpy(&dv1, &v1, 8);
+  memcpy(&dv2, &v2, 8);
+  return _FortranAioOutputComplex64(v0, dv1, dv2);
+}
+extern bool V_FortranAioOutputLogical(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  void *v0 = va_arg(args, void *);
+  uint32_t v1 = va_arg(args, uint32_t);
+  va_end(args);
+  v0 = _list_started_cookie;
+  return _FortranAioOutputLogical(v0, v1);
+}
+extern void V_FortranAAbort(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  va_end(args);
+  _FortranAAbort();
+  // Now return to device to run abort from stub
+}
+extern void V_FortranAStopStatementText(void *fnptr, ...) {
+  va_list args;
+  va_start(args, fnptr);
+  char *errmsg = va_arg(args, char *);
+  int64_t a1 = va_arg(args, int64_t);
+  uint32_t a2 = va_arg(args, uint32_t);
+  uint32_t a3 = va_arg(args, uint32_t);
+  va_end(args);
+  bool b2 = (bool)a2;
+  bool b3 = (bool)a3;
+  _FortranAStopStatementText(errmsg, a1, b2, b3);
+}
+}
+
+// Static vars used to defer functions to reorder execution by thread and team.
+static uint32_t _deferred_fn_count = 0;
+static uint32_t _deferred_begin_statements = 0;
+static uint32_t _deferred_end_statements = 0;
+static uint64_t _max_num_threads = 0;
+static uint64_t _max_num_teams = 0;
+// structure for deferred functions
+typedef struct {
+  uint32_t NumArgs;    // The number of args in arg_array
+  void *fnptr;         // The function pointer for this index
+  uint64_t fn_idx;     // The function index, good for debug
+  uint32_t dfnid;      // The dvoideferred function id, in order received
+  uint64_t *arg_array; // ptr to malloced arg_array
+  char *c_ptr;         // ptr to null terminated char string
+  uint64_t thread_num;
+  uint64_t num_threads;
+  uint64_t team_num;
+  uint64_t num_teams;
+  uint64_t *return_value; // pointer to where return value is copied
+} deferred_entry_t;
+
+static std::vector<deferred_entry_t *>*  _deferred_fns_ptr;
+// static std::list<deferred_entry_t *> _deferred_fns;
+
+static service_rc fortran_rt_service(char *buf, size_t bufsz,
+                                     uint64_t *return_value) {
+  if (bufsz == 0)
+    return _RC_SUCCESS;
+
+  // Get 6 values needed to unpack the buffer
+  int *datalen = (int *)buf;
+  int NumArgs = *((int *)(buf + sizeof(int)));
+  size_t data_not_used =
+      (size_t)(*datalen) - ((size_t)(2 + NumArgs) * sizeof(int));
+  char *keyptr = buf + (2 * sizeof(int));
+  char *dataptr = keyptr + (NumArgs * sizeof(int));
+  char *strptr = buf + (size_t)*datalen;
+
+  // skip the function pointer arg including any align buffer
+  if (((size_t)dataptr) % (size_t)8) {
+    dataptr += 4;
+    data_not_used -= 4;
+  }
+  void *fnptr = getfnptr(dataptr);
+  NumArgs--;
+  keyptr += 4;
+  dataptr += 8;
+  data_not_used -= 4;
+
+  if (NumArgs <= 0)
+    return _RC_ERROR_INVALID_REQUEST;
+
+
+  uint64_t *a[MAXVARGS];
+  if (hostrpc_build_vargs_array(NumArgs, keyptr, dataptr, strptr,
+                                &data_not_used, a) != _RC_SUCCESS)
+    return _RC_ERROR_INVALID_REQUEST;
+
+  // std::list<deferred_entry_t *> _deferred_fns;
+  if (!_deferred_fns_ptr)
+    _deferred_fns_ptr = new std::vector<deferred_entry_t *> ;
+
+  char *c_ptr = nullptr;
+  bool defer_for_reorder = true;
+  bool run_deferred_functions = false;
+  uint64_t DeviceRuntime_idx = (uint64_t)fnptr;
+  switch (DeviceRuntime_idx) {
+  case _FortranAioBeginExternalListOutput_idx: {
+    _deferred_begin_statements++;
+    fnptr = (void *)V_FortranAioBeginExternalListOutput;
+    size_t slen = std::strlen((char *)a[5]) + 1;
+    c_ptr = (char *)aligned_alloc(sizeof(uint64_t *), slen);
+    if (!c_ptr)
+      fprintf(stderr, "MALLOC FAILED for c_ptr size:%ld \n", slen);
+    std::strncpy(c_ptr, (char *)a[5], slen - 1);
+    c_ptr[slen - 1] = (char)0;
+    a[5] = (uint64_t *)c_ptr;
+    break;
+  }
+  case _FortranAioOutputAscii_idx: {
+    fnptr = (void *)V_FortranAioOutputAscii;
+
+    size_t slen = (size_t)a[6] + 1;
+    c_ptr = (char *)aligned_alloc(sizeof(uint64_t *), slen);
+    if (!c_ptr)
+      fprintf(stderr, "MALLOC FAILED for c_ptr size:%ld \n", slen);
+    std::strncpy(c_ptr, (char *)a[5], slen - 1);
+    c_ptr[slen - 1] = (char)0;
+    a[5] = (uint64_t *)c_ptr;
+
+    break;
+  }
+  case _FortranAioOutputInteger32_idx: {
+    fnptr = (void *)V_FortranAioOutputInteger32;
+    break;
+  }
+  case _FortranAioEndIoStatement_idx: {
+    _deferred_end_statements++;
+    fnptr = (void *)V_FortranAioEndIoStatement;
+    // We cannot use last tread and team number to trigger running deferred
+    // functions because its warp could finish early (out of order). So, if
+    // this is the last FortranAioEndIoStatement by count of begin statements,
+    // then run the deferred functions ordered by team and thread number.
+    if (_deferred_end_statements == _deferred_begin_statements)
+      run_deferred_functions = true;
+    break;
+  }
+  case _FortranAioOutputInteger8_idx: {
+    fnptr = (void *)V_FortranAioOutputInteger8;
+    break;
+  }
+  case _FortranAioOutputInteger16_idx: {
+    fnptr = (void *)V_FortranAioOutputInteger16;
+    break;
+  }
+  case _FortranAioOutputInteger64_idx: {
+    fnptr = (void *)V_FortranAioOutputInteger64;
+    break;
+  }
+  case _FortranAioOutputReal32_idx: {
+    fnptr = (void *)V_FortranAioOutputReal32;
+    break;
+  }
+  case _FortranAioOutputReal64_idx: {
+    fnptr = (void *)V_FortranAioOutputReal64;
+    break;
+  }
+  case _FortranAioOutputComplex32_idx: {
+    fnptr = (void *)V_FortranAioOutputComplex32;
+    break;
+  }
+  case _FortranAioOutputComplex64_idx: {
+    fnptr = (void *)V_FortranAioOutputComplex64;
+    break;
+  }
+  case _FortranAioOutputLogical_idx: {
+    fnptr = (void *)V_FortranAioOutputLogical;
+    break;
+  }
+  case _FortranAAbort_idx: {
+    defer_for_reorder = false;
+    fnptr = (void *)V_FortranAAbort;
+    break;
+  }
+  case _FortranAStopStatementText_idx: {
+    defer_for_reorder = false;
+    fnptr = (void *)V_FortranAStopStatementText;
+    break;
+  }
+  case _FortranAio_INVALID:
+  default: {
+    defer_for_reorder = false;
+    break;
+  }
+  } // end of switch
+
+  if (defer_for_reorder) {
+    _deferred_fn_count++;
+    deferred_entry_t *q = new deferred_entry_t;
+
+    q->dfnid = _deferred_fn_count - 1;
+    q->thread_num = (uint64_t)a[0];
+    q->num_threads = (uint64_t)a[1];
+    _max_num_threads =
+        (q->num_threads > _max_num_threads) ? q->num_threads : _max_num_threads;
+    q->team_num = (uint64_t)a[2];
+    q->num_teams = (uint64_t)a[3];
+    _max_num_teams =
+        (q->num_teams > _max_num_teams) ? q->num_teams : _max_num_teams;
+    q->NumArgs = NumArgs - 4;
+    q->fnptr = fnptr;
+    q->fn_idx = DeviceRuntime_idx;
+    uint64_t *arg_array = (uint64_t *)aligned_alloc(
+        sizeof(uint64_t), (NumArgs - 4) * sizeof(uint64_t));
+    if (!arg_array)
+      fprintf(stderr, " MALLOC FAILED for arg_array size:%ld \n",
+              sizeof(uint64_t) * (NumArgs - 4));
+    for (int32_t i = 0; i < NumArgs - 4; i++) {
+      uint64_t val = (uint64_t)a[i + 4];
+      arg_array[i] = val;
+    }
+    q->arg_array = arg_array;
+    q->return_value = (uint64_t *)return_value;
+    q->c_ptr = c_ptr;
+    _deferred_fns_ptr->push_back(q);
+  } else {
+    // non deferred functions get a return_value
+    if (hostrpc_call_fnptr<uint64_t, hostexec_uint64_t>(
+            NumArgs - 4, fnptr, &a[4], return_value) != _RC_SUCCESS)
+      return _RC_ERROR_INVALID_REQUEST;
+  }
+
+  if (run_deferred_functions) {
+    // This specific team and thread ordering does not reflect the 
+    // actual non-deterministic ordering.
+    for (uint32_t team_num = 0; team_num < _max_num_teams; team_num++) {
+      for (uint32_t thread_num = 0; thread_num < _max_num_threads;
+           thread_num++) {
+        for (auto q : *_deferred_fns_ptr) {
+          if ((thread_num == q->thread_num) && (team_num == q->team_num)) {
+            for (uint32_t i = 0; i < q->NumArgs; i++)
+              a[i] = (uint64_t *)q->arg_array[i];
+            service_rc rc = hostrpc_call_fnptr<uint64_t, hostexec_uint64_t>(
+                q->NumArgs, q->fnptr, a, q->return_value);
+            if (rc != _RC_SUCCESS) {
+              fprintf(stderr, "    BAD RETURN FROM hostrpc_call_fnptr %d\n",
+                      rc);
+              return _RC_ERROR_INVALID_REQUEST;
+            }
+          }
+	  // Only the return value for the last end statement is returned.
+	  return_value = q->return_value;
+        }
+      }
+    }
+
+    //  Reset static deferred function counters and free memory
+    for (auto q : *_deferred_fns_ptr) {
+      if (q->c_ptr)
+        free(q->c_ptr);
+      free(q->arg_array);
+      delete q;
+    }
+    _deferred_fns_ptr->clear();
+    _deferred_fn_count = 0;
+    _deferred_begin_statements = 0;
+    _deferred_end_statements = 0;
+    _max_num_threads = 0;
+    _max_num_teams = 0;
+    delete _deferred_fns_ptr;
+  } // end run_deferred_functions 
+
+  return _RC_SUCCESS;
+} // end fortran_rt_service
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/execute_service.h llvm-project/offload/plugins-nextgen/amdgpu/hostexec/execute_service.h
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/execute_service.h	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/execute_service.h	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,56 @@
+//===---- execute_service.h - header file for execute_service -------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef EXECUTE_SERVICE_H
+#define EXECUTE_SERVICE_H
+
+#include <cstdlib>
+#include <stdint.h>
+
+// Error return codes for service handler functions
+typedef enum service_rc {
+  _RC_SUCCESS = 0,
+  _RC_STATUS_UNKNOWN = 1,
+  _RC_STATUS_ERROR = 2,
+  _RC_STATUS_TERMINATE = 3,
+  _RC_DATA_USED_ERROR = 4,
+  _RC_ADDINT_ERROR = 5,
+  _RC_ADDFLOAT_ERROR = 6,
+  _RC_ADDSTRING_ERROR = 7,
+  _RC_UNSUPPORTED_ID_ERROR = 8,
+  _RC_INVALID_ID_ERROR = 9,
+  _RC_ERROR_INVALID_REQUEST = 10,
+  _RC_EXCEED_MAXVARGS_ERROR = 11,
+  _RC_INVALIDSERVICE_ERROR = 12,
+  _RC_ERROR_MEMFREE = 13,
+  _RC_ERROR_CONSUMER_ACTIVE = 14,
+  _RC_ERROR_CONSUMER_INACTIVE = 15,
+  _RC_ERROR_CONSUMER_LAUNCH_FAILED = 16,
+  _RC_ERROR_SERVICE_UNKNOWN = 17,
+  _RC_ERROR_INCORRECT_ALIGNMENT = 18,
+  _RC_ERROR_NULLPTR = 19,
+  _RC_ERROR_WRONGVERSION = 20,
+  _RC_ERROR_OLDHOSTVERSIONMOD = 21,
+  _RC_ERROR_HSAFAIL = 22,
+  _RC_ERROR_ZEROPACKETS = 23,
+  _RC_ERROR_ALIGNMENT = 24,
+} service_rc;
+
+// helper functions defined in <arch>-hostrpc.cpp used by execute_service
+service_rc host_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc device_malloc(void **mem, size_t size, uint32_t device_id);
+service_rc host_device_mem_free(void *mem);
+void thread_abort(service_rc);
+
+typedef struct {
+  uint64_t slots[64][8];
+} payload_t;
+
+void execute_service(uint32_t service_id, uint32_t devid, uint64_t *payload);
+
+#endif
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/hostexec.h llvm-project/offload/plugins-nextgen/amdgpu/hostexec/hostexec.h
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/hostexec.h	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/hostexec.h	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,55 @@
+//
+// hostexec.h: Headers for hostexec device stubs
+//
+#ifndef __HOSTEXEC_H__
+#define __HOSTEXEC_H__
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+#include <stdint.h>
+#include <stdio.h>
+
+typedef void hostexec_t(void *, ...);
+typedef uint32_t hostexec_uint_t(void *, ...);
+typedef uint64_t hostexec_uint64_t(void *, ...);
+typedef double hostexec_double_t(void *, ...);
+typedef float hostexec_float_t(void *, ...);
+typedef int hostexec_int_t(void *, ...);
+typedef long hostexec_long_t(void *, ...);
+
+#if defined(__NVPTX__) || defined(__AMDGCN__)
+
+// Device interfaces for user-callable hostexec functions
+EXTERN void hostexec(void *fnptr, ...);
+EXTERN uint32_t hostexec_uint(void *fnptr, ...);
+EXTERN uint64_t hostexec_uint64(void *fnptr, ...);
+EXTERN uint64_t hostexec_fortrt(void *fnptr, ...);
+EXTERN double hostexec_double(void *fnptr, ...);
+EXTERN float hostexec_float(void *fnptr, ...);
+EXTERN int hostexec_int(void *fnptr, ...);
+EXTERN long hostexec_long(void *fnptr, ...);
+
+#else
+
+
+//  On host pass, simply drop the hostexec wrapper. Technically,
+//  host passes should not see these hostexec functions
+#define hostexec_uint(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec_uint64(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec_double(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec_float(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec_int(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec_long(fn, ...) fn(fn, __VA_ARGS__)
+#define hostexec(fn, ...) fn(fn, __VA_ARGS__)
+// FIXME: this should be defined as an aborting function
+//        if ever seen on a host pass because first arg
+//        is an enum for fortrt, not a function pointer
+#define hostexec_fortrt(fn, ...) fn(fn, __VA_ARGS__)
+
+#endif
+
+#endif // __HOSTEXEC_H__
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/hostexec_internal.h llvm-project/offload/plugins-nextgen/amdgpu/hostexec/hostexec_internal.h
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/hostexec_internal.h	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/hostexec_internal.h	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,64 @@
+#ifndef __HOSTEXEC_INTERNAL_H__
+#define __HOSTEXEC_INTERNAL_H__
+
+/*
+ *   hostexec_internal.h:
+
+MIT License
+
+Copyright © 2020 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is furnished
+to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+*/
+
+#if defined(__cplusplus)
+#define EXTERN extern "C"
+#else
+#define EXTERN extern
+#endif
+
+#define NOINLINE __attribute__((noinline))
+
+
+//  These are the interfaces for the device stubs emitted
+//  by EmitHostexecAllocAndExecFns in CGGPUBuiltin.cpp
+EXTERN char *printf_allocate(uint32_t bufsz);
+EXTERN int printf_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *fprintf_allocate(uint32_t bufsz);
+EXTERN int fprintf_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_allocate(uint32_t bufsz);
+EXTERN void hostexec_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_uint_allocate(uint32_t bufsz);
+EXTERN uint32_t hostexec_uint_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_uint64_allocate(uint32_t bufsz);
+EXTERN uint64_t hostexec_uint64_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_double_allocate(uint32_t bufsz);
+EXTERN double hostexec_double_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_int_allocate(uint32_t bufsz);
+EXTERN int hostexec_int_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_long_allocate(uint32_t bufsz);
+EXTERN long hostexec_long_execute(char *bufptr, uint32_t bufsz);
+EXTERN char *hostexec_float_allocate(uint32_t bufsz);
+EXTERN float hostexec_float_execute(char *bufptr, uint32_t bufsz);
+
+// This device runtime utility function is needed for variable length strings.
+EXTERN uint32_t __strlen_max(char *instr, uint32_t maxstrlen);
+
+#endif // __HOSTEXEC_INTERNAL_H__
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/urilocator.h llvm-project/offload/plugins-nextgen/amdgpu/hostexec/urilocator.h
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/hostexec/urilocator.h	1969-12-31 18:00:00.000000000 -0600
+++ llvm-project/offload/plugins-nextgen/amdgpu/hostexec/urilocator.h	2024-09-25 08:28:31.616214109 -0500
@@ -0,0 +1,68 @@
+//===--- UriLocator.h: Schema of URI Locator  -----------------------------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+
+/* Copyright (c) 2023 Advanced Micro Devices, Inc.
+
+ Permission is hereby granted, free of charge, to any person obtaining a copy
+ of this software and associated documentation files (the "Software"), to deal
+ in the Software without restriction, including without limitation the rights
+ to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ copies of the Software, and to permit persons to whom the Software is
+ furnished to do so, subject to the following conditions:
+
+ The above copyright notice and this permission notice shall be included in
+ all copies or substantial portions of the Software.
+
+ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ THE SOFTWARE.
+*/
+
+#ifndef URILOCATOR_H
+#define URILOCATOR_H
+
+#if __has_include("hsa/hsa.h")
+#include "hsa/hsa_ven_amd_loader.h"
+#else
+#include "hsa_ven_amd_loader.h"
+#endif
+#include <string>
+#include <vector>
+
+class UriLocator {
+
+public:
+  struct UriInfo {
+    std::string uriPath;
+    int64_t loadAddressDiff;
+  };
+
+  struct UriRange {
+    uint64_t startAddr_, endAddr_;
+    int64_t elfDelta_;
+    std::string Uri_;
+  };
+
+  bool init_ = false;
+  std::vector<UriRange> rangeTab_;
+  hsa_ven_amd_loader_1_03_pfn_t fn_table_;
+
+  hsa_status_t createUriRangeTable();
+
+  ~UriLocator() {}
+
+  UriInfo lookUpUri(uint64_t device_pc);
+  std::pair<uint64_t, uint64_t> decodeUriAndGetFd(UriInfo &uri_path,
+                                                  int *uri_fd);
+};
+
+#endif
diff -Naur -x .git llvm-project.atd/offload/plugins-nextgen/amdgpu/src/rtl.cpp llvm-project/offload/plugins-nextgen/amdgpu/src/rtl.cpp
--- llvm-project.atd/offload/plugins-nextgen/amdgpu/src/rtl.cpp	2024-09-25 10:05:15.830432503 -0500
+++ llvm-project/offload/plugins-nextgen/amdgpu/src/rtl.cpp	2024-09-25 09:54:16.312683109 -0500
@@ -94,6 +94,14 @@
 
 namespace hsa_utils {
 
+extern "C" {
+uint64_t hostrpc_assign_buffer(hsa_agent_t Agent, hsa_queue_t *ThisQ,
+                               uint32_t DeviceId,
+                               hsa_amd_memory_pool_t HostMemoryPool,
+                               hsa_amd_memory_pool_t DevMemoryPool);
+hsa_status_t hostrpc_terminate();
+}
+
 /// Iterate elements using an HSA iterate function. Do not use this function
 /// directly but the specialized ones below instead.
 template <typename ElemTy, typename IterFuncTy, typename CallbackTy>
@@ -496,6 +504,9 @@
     return It->second;
   }
 
+  /// Does device image contain Symbol
+  bool hasDeviceSymbol(GenericDeviceTy &Device, StringRef SymbolName) const;
+
 private:
   /// The exectuable loaded on the agent.
   hsa_executable_t Executable;
@@ -508,7 +519,12 @@
 /// generic kernel class.
 struct AMDGPUKernelTy : public GenericKernelTy {
   /// Create an AMDGPU kernel with a name and an execution mode.
-  AMDGPUKernelTy(const char *Name) : GenericKernelTy(Name) {}
+  AMDGPUKernelTy(const char *Name, GenericGlobalHandlerTy &Handler)
+      : GenericKernelTy(Name),
+        OMPX_DisableHostExec("LIBOMPTARGET_DISABLE_HOST_EXEC", false),
+        ServiceThreadDeviceBufferGlobal("service_thread_buf",
+			sizeof(uint64_t)),
+        HostServiceBufferHandler(Handler) {}
 
   /// Initialize the AMDGPU kernel.
   Error initImpl(GenericDeviceTy &Device, DeviceImageTy &Image) override {
@@ -560,6 +576,14 @@
       INFO(OMP_INFOTYPE_PLUGIN_KERNEL, Device.getDeviceId(),
            "Could not read extra information for kernel %s.", getName());
 
+    NeedsHostServices =
+        AMDImage.hasDeviceSymbol(Device, "__needs_host_services");
+    if (NeedsHostServices && !OMPX_DisableHostExec) {
+      if (auto Err = HostServiceBufferHandler.getGlobalMetadataFromDevice(
+              Device, AMDImage, ServiceThreadDeviceBufferGlobal))
+        return Err;
+    }
+
     return Plugin::success();
   }
 
@@ -588,6 +612,9 @@
   /// Indicates whether or not we need to set up our own private segment size.
   bool usesDynamicStack() const { return DynamicStack; }
 
+  /// Envar to disable host-exec thread creation.
+  BoolEnvar OMPX_DisableHostExec;
+
 private:
   /// The kernel object to execute.
   uint64_t KernelObject;
@@ -603,6 +630,16 @@
 
   /// Additional Info for the AMD GPU Kernel
   std::optional<offloading::amdgpu::AMDGPUKernelMetaData> KernelInfo;
+
+  /// Indicate whether this Kernel requires host services
+  bool NeedsHostServices;
+
+  /// Global for host service device thread buffer
+  GlobalTy ServiceThreadDeviceBufferGlobal;
+
+  /// Global handler for hostservices buffer
+  GenericGlobalHandlerTy &HostServiceBufferHandler;
+
 };
 
 /// Class representing an HSA signal. Signals are used to define dependencies
@@ -786,6 +823,12 @@
     return pushBarrierImpl(OutputSignal, InputSignal1, InputSignal2);
   }
 
+  /// Return the pointer to the underlying HSA queue
+  hsa_queue_t *getHsaQueue() {
+    assert(Queue && "HSA Queue initialized");
+    return Queue;
+  }
+
 private:
   /// Push a barrier packet that will wait up to two input signals. Assumes the
   /// the queue lock is acquired.
@@ -1129,6 +1172,7 @@
       return Err;
 
     // Push a barrier into the queue with both input signals.
+    DP("Using Queue: %p with HSA Queue: %p\n", Queue, Queue->getHsaQueue());
     return Queue->pushBarrier(OutputSignal, InputSignal, OtherSignal);
   }
 
@@ -1218,6 +1262,8 @@
   /// Deinitialize the stream's signals.
   Error deinit() { return Plugin::success(); }
 
+  hsa_queue_t *getHsaQueue() { return Queue->getHsaQueue(); }
+
   /// Attach an RPC server to this stream.
   void setRPCServer(RPCServerTy *Server) { RPCServer = Server; }
 
@@ -1745,6 +1791,9 @@
 
     return Plugin::success();
   }
+  AMDGPUMemoryPoolTy *getCoarseGrainedMemoryPool() {
+    return CoarseGrainedMemoryPools[0];
+  }
 
   /// Retrieve and construct all memory pools from the device agent(s).
   virtual Error retrieveAllMemoryPools() = 0;
@@ -2117,7 +2166,8 @@
     if (!AMDGPUKernel)
       return Plugin::error("Failed to allocate memory for AMDGPU kernel");
 
-    new (AMDGPUKernel) AMDGPUKernelTy(Name);
+    //new (AMDGPUKernel) AMDGPUKernelTy(Name);
+    new (AMDGPUKernel) AMDGPUKernelTy(Name, Plugin.getGlobalHandler());
 
     return *AMDGPUKernel;
   }
@@ -2808,7 +2858,8 @@
       return Plugin::success();
 
     // Allocate and construct the AMDGPU kernel.
-    AMDGPUKernelTy AMDGPUKernel(KernelName);
+    //AMDGPUKernelTy AMDGPUKernel(KernelName);
+    AMDGPUKernelTy AMDGPUKernel(KernelName, Plugin.getGlobalHandler());
     if (auto Err = AMDGPUKernel.init(*this, Image))
       return Err;
 
@@ -2980,6 +3031,7 @@
   hsa_agent_t Agent = AMDGPUDevice.getAgent();
 
   hsa_executable_symbol_t Symbol;
+  // printf("= = = > findDeviceSymbol for %s\n",SymbolName.data());
   hsa_status_t Status = hsa_executable_get_symbol_by_name(
       Executable, SymbolName.data(), &Agent, &Symbol);
   if (auto Err = Plugin::check(
@@ -2990,6 +3042,17 @@
   return Symbol;
 }
 
+bool AMDGPUDeviceImageTy::hasDeviceSymbol(GenericDeviceTy &Device,
+                                          StringRef SymbolName) const {
+  AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(Device);
+  hsa_agent_t Agent = AMDGPUDevice.getAgent();
+  hsa_executable_symbol_t Symbol;
+  // printf("= = = > hasDeviceSymbol for %s\n",SymbolName.data());
+  hsa_status_t Status = hsa_executable_get_symbol_by_name(
+      Executable, SymbolName.data(), &Agent, &Symbol);
+  return (Status == HSA_STATUS_SUCCESS);
+}
+
 template <typename ResourceTy>
 Error AMDGPUResourceRef<ResourceTy>::create(GenericDeviceTy &Device) {
   if (Resource)
@@ -3143,6 +3206,7 @@
 
   /// Deinitialize the plugin.
   Error deinitImpl() override {
+    hsa_utils::hostrpc_terminate();
     // The HSA runtime was not initialized, so nothing from the plugin was
     // actually initialized.
     if (!Initialized)
@@ -3353,11 +3417,35 @@
   if (LaunchParams.Size)
     std::memcpy(AllArgs, LaunchParams.Data, LaunchParams.Size);
 
+  uint64_t Buffer = 0;
   AMDGPUDeviceTy &AMDGPUDevice = static_cast<AMDGPUDeviceTy &>(GenericDevice);
 
   AMDGPUStreamTy *Stream = nullptr;
   if (auto Err = AMDGPUDevice.getStream(AsyncInfoWrapper, Stream))
     return Err;
+  if (NeedsHostServices) {
+    int32_t DevID = AMDGPUDevice.getDeviceId();
+    hsa_amd_memory_pool_t HostMemPool =
+        HostDevice.getFineGrainedMemoryPool().get();
+    hsa_amd_memory_pool_t DeviceMemPool =
+        AMDGPUDevice.getCoarseGrainedMemoryPool()->get();
+    hsa_queue_t *HsaQueue = Stream->getHsaQueue();
+    Buffer = hsa_utils::hostrpc_assign_buffer(AMDGPUDevice.getAgent(), HsaQueue,
+                                          DevID, HostMemPool, DeviceMemPool);
+    GlobalTy ServiceThreadHostBufferGlobal("service_thread_buf",
+                                           sizeof(uint64_t), &Buffer);
+    if (auto Err = HostServiceBufferHandler.writeGlobalToDevice(
+            AMDGPUDevice, ServiceThreadHostBufferGlobal,
+            ServiceThreadDeviceBufferGlobal)) {
+      DP("Missing symbol %s, continue execution anyway.\n",
+         ServiceThreadHostBufferGlobal.getName().data());
+      consumeError(std::move(Err));
+    }
+    DP("Hostrpc buffer allocated at %p and service thread started\n",
+       (void *)Buffer);
+  } else {
+    DP("No hostrpc buffer or service thread required\n");
+  }
 
   // If this kernel requires an RPC server we attach its pointer to the stream.
   if (GenericDevice.getRPCServer())
