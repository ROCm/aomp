######################################################################
# Example configuration file for AOMP LLVM/Clang compiler.
#
# Defines: "model" => "mpi", "omp", "omp_target", omp_host_target"
#              default "mpi"
#          "label" => ext base label,
#              default "clang"
#          "gputype" => "host" , "x86", "gfx900", "gfx906", "gfx908"
#              default "host"
# Example runhpc commands
#
# MPI-only Command:
# runhpc -c amdgpu_clang.cfg -I -l -n 1 -T base --define model=mpi --threads=1 --ranks=16 -i test 628
#
# MPI+OpenMP Command:
# runhpc -c amdgpu_clang.cfg -I -l -n 1 -T base --define model=omp --threads=16--ranks=1 -i test 628
#
# MPI+OpenMP target offload Command:
# runhpc -c amdgpu_clang.cfg -I -l -n 1 -T base --define model=omp_target --define gputype=gfx908 --threads=1 --ranks=4 -i test 628
#
# MPI+OpenMP target offload to host Command:
# runhpc -c amdgpu_clang.cfg -I -l -n 1 -T base --define model=omp_host_target --define gputype=x86 --threads=16 --ranks=1 -i test 628
#
#######################################################################

%ifndef %{label}         # IF label is not set use clang
%   define label gnu
%endif

%ifndef %{model}       # IF model is not set use mpi
%   define model mpi
%endif

%ifndef %{gputype}
%   define gputype host
%endif
######################################################################
# The header section of the config file.  Must appear
# before any instances of "section markers" (see below)
#
# ext = how the binaries you generated will be identified
# tune = specify "base" or "peak" or "all"

label         = %{label}_%{model}_%{gputype}
tune          = base
output_format = text
use_submit_for_speed = 1

makeflags = -j 16

#strict_rundir_verify=0

include: desc_amdgpu.inc
flagsurl=$[top]/config/flags/amd2021_flags.xml

default:
CC           = mpicc
CXX          = mpicxx
FC           = mpif90
sw_compiler  = Siemens

CC_VERSION_OPTION  = --version
CXX_VERSION_OPTION = --version
FC_VERSION_OPTION  = --version

#preENV_OMP_PROC_BIND=true
MPIRUN_OPTS = --bind-to none #socket # core
submit = mpirun ${MPIRUN_OPTS} -np $ranks $command

#######################################################################

default=base=default:

# portability & libraries
OPTIMIZE += -O2

EXTRA_FLIBS              = -lm
EXTRA_LIBS = -lopen-pal
PORTABILITY_LIBS = -lm
#FPPPORTABILITY +=  -DSPEC_USE_MPIFH -I${MPI}/include/

%if %{model} eq 'mpi'
  pmodel=MPI
  MPIRUN_OPTS += --mca topo basic
  submit = mpirun ${MPIRUN_OPTS} -np $ranks $command
%endif

%if %{model} eq 'omp'
  pmodel=OMP
  OPTIMIZE += -fopenmp
  #FOPTIMIZE     = -O3 -ffast-math -flto -march=znver3 
  MPIRUN_OPTS = --bind-to  core
  MPIRUN_OPTS += --map-by ppr:1:numa:pe=8 # 16 cores per numa
  #MPIRUN_OPTS += --map-by ppr:1:numa:pe=64 # 64 cores per numa  
  submit = mpirun ${MPIRUN_OPTS} -np $ranks $command
%endif

%if %{model} eq 'omp_target'
  pmodel=TGT
  MPIRUN_OPTS = 
  #MPIRUN_OPTS = --bind-to socket
  #MPIRUN_OPTS = --bind-to  core  
  MPIRUN_OPTS = --bind-to none 
  #submit = mpirun ${MPIRUN_OPTS} -np $ranks $command
  submit = mpirun ${MPIRUN_OPTS} -np $ranks $command

  OPTIMIZE += -fopenmp
  COPTIMIZE += -foffload="-march=%{gputype}" -ffast-math -foffload=-lm
  CXXOPTIMIZE += -foffload="-march=%{gputype}"  -ffast-math -foffload=-lm
  FOPTIMIZE += -foffload="-march=%{gputype} -lgfortran"  -ffree-line-length-0 -ffast-math -DSPEC_NO_DEV_SUPPORT -foffload=-lm -foffload=-mstack-size=327680

  513.soma_t,613.soma_s:
    PORTABILITY += -DSPEC_NO_VAR_ARRAY_REDUCE

%endif

%if %{model} eq 'omp_host_target'
  pmodel=TGT
  521.miniswp_t,621.miniswp_s:
    PORTABILITY += -DSPEC_USE_HOST_THREADS
%endif

# No peak flags set, so make peak use the same flags as base
default=peak=default:
basepeak=1

